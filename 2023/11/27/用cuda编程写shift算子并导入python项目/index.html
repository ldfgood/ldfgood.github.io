<!DOCTYPE HTML>
<html>

<head>
	<link rel="bookmark"  type="image/x-icon"  href="/img/logo_miccall.png"/>
	<link rel="shortcut icon" href="/img/logo_miccall.png">
	
			    <title>
    锋哥的个人网站
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
    <meta name="keywords" content="LDF" />
    
    	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	 
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/bg.jpg') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
<meta name="generator" content="Hexo 6.3.0"></head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css" />
<link rel="stylesheet" href="/css/typo.css" />
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">LDF</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">主页</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1">分类</a>
	                    <ul class="submenu">
	                        <li>
	                        
	                    </ul>
	        </li>
	        
	        <!-- archives  归档   --> 
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/about/" title="简历">
		                简历
		            </a>
		        </li>
		        
		        <li>
		            <a href="/group/" title="团队">
		                团队
		            </a>
		        </li>
		        
		        <li>
		            <a href="/gallery/" title="图库">
		                图库
		            </a>
		        </li>
		        
		        <li>
		            <a href="/tag/" title="标签">
		                标签
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/ldfgood" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
                    <li>
                        <a title="csdn" href="https://blog.csdn.net/weixin_64089712" target="_blank" rel="noopener">
                            <i class="icon fa fa-csdn"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img" style="height: 25rem;background-image: url();background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2 >用cuda编程写shift算子并导入python项目</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <hr>
<h1 id="简要介绍"><a href="#简要介绍" class="headerlink" title="简要介绍"></a>简要介绍</h1><p>想要看更多内容，可以看看我的<a href="https://blog.ldf.icu/">个人博客 (ldf.icu)</a></p>
<p>最近看了shift-gcn的论文并学习了他的代码</p>
<p>收获最大的就是用它的作为例子学习了一点CUDA编程,基于CUDA编程可以利用GPUs的并行计算引擎来更加高效地解决比较复杂的计算难题。</p>
<p>对于一种典型的扩展情况，比如我们要设计一个全新的C++底层算子，其过程其实就三步：</p>
<p>第一步：使用C++编写算子的<strong>forward</strong>函数和<strong>backward</strong>函数</p>
<p>第二步：将该算子的<strong>forward</strong>函数和<strong>backward</strong>函数使用pybind11绑定到python上</p>
<p>第三步：使用<strong>setuptools</strong>&#x2F;<strong>JIT</strong>&#x2F;<strong>CMake</strong>编译打包C++工程为so文件</p>
<p>项目结构如下所示</p>
<p><img src="/2023/11/27/%E7%94%A8cuda%E7%BC%96%E7%A8%8B%E5%86%99shift%E7%AE%97%E5%AD%90%E5%B9%B6%E5%AF%BC%E5%85%A5python%E9%A1%B9%E7%9B%AE/image-20231128204749676.png" alt="image-20231128204749676"></p>
<h1 id="代码编写"><a href="#代码编写" class="headerlink" title="代码编写"></a>代码编写</h1><p>shift-gcn中用CUDA编程实现的shift算子，用于时间的shift操作，公式如下所示：</p>
<p><img src="/2023/11/27/%E7%94%A8cuda%E7%BC%96%E7%A8%8B%E5%86%99shift%E7%AE%97%E5%AD%90%E5%B9%B6%E5%AF%BC%E5%85%A5python%E9%A1%B9%E7%9B%AE/image-20230719210751210.png" alt="image-20230719210751210"></p>
<p><strong>先看看代码，做了注释说明，精华都在注释里</strong></p>
<h2 id="shift-cuda-kernel-cu"><a href="#shift-cuda-kernel-cu" class="headerlink" title="shift_cuda_kernel.cu"></a>shift_cuda_kernel.cu</h2><p>咱们需要些shift算子的forward函数和backward函数，咱们先在shift_cuda_kernel.cu写完底层的代码之后再封装到shift_cuda.cpp中。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;ATen/ATen.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> &#123;</span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;<span class="comment">//c++的模板声明方法，使得函数、类或数据结构可以在不同的数据类型下工作</span></span><br><span class="line">  <span class="function">__global__ <span class="type">void</span> <span class="title">shift_cuda_forward_kernel</span><span class="params">(<span class="comment">//shift正向传递的操作的核函数</span></span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="type">const</span> <span class="type">scalar_t</span>* __restrict__ input,</span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="comment">//传入的特征x，const scalar_t* __restrict__ 通过这种方式传入数据是cuda编程的特性，每个线程负责处理不同的数据。</span></span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="comment">//通过使用指针，可以在 GPU 设备上直接访问底层数据，从而提高计算效率。</span></span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="type">scalar_t</span>* output,<span class="comment">//保存结果</span></span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="type">scalar_t</span>* xpos, <span class="comment">//骨架维度的偏移值数组</span></span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="type">scalar_t</span>* ypos,<span class="comment">//时间维度的偏移值数组</span></span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="type">const</span> <span class="type">int</span> batch,<span class="comment">//batch数量</span></span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="type">const</span> <span class="type">int</span> channel,<span class="comment">//通道数量</span></span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="type">const</span> <span class="type">int</span> bottom_height,<span class="comment">//时间维度的大小</span></span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="type">const</span> <span class="type">int</span> bottom_width,<span class="comment">//骨架点的数量，如果使用coco keypoints格式就是17</span></span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="type">const</span> <span class="type">int</span> top_height,<span class="comment">//时间大小除以步长（stride）</span></span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="type">const</span> <span class="type">int</span> top_width,<span class="comment">//骨架点数量</span></span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="type">const</span> <span class="type">int</span> stride)</span> <span class="comment">//时间维度的步长</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> index = blockIdx.x * blockDim.x + threadIdx.x;<span class="comment">//通过块id乘以块里边的线程数，加上在该块的线程的id，计算出是第几个进程</span></span><br><span class="line">      <span class="comment">//（这里是一维的grid和一维的block，所以可以只用x计算索引，2维和三维有另外的计算方式）</span></span><br><span class="line">      <span class="comment">//cuda编程中有块和线程，一个块里有多个线程，一般用一个线程运行一个核函数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (index &lt; batch*channel*top_height*top_width)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> top_sp_dim = top_height * top_width;</span><br><span class="line">        <span class="comment">//为骨架数乘以时间除以步长的值一个通道考虑步长所有数据的个数</span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> bottom_sp_dim = bottom_height * bottom_width;</span><br><span class="line">        <span class="comment">//时间乘以骨架点数，一个通道所有数据的个数</span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> n = index/(channel * top_sp_dim);<span class="comment">//第几个批次（考虑步长）       </span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> idx = index%(channel * top_sp_dim);<span class="comment">//批次中第几个数据（考虑步长）</span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> c_out = idx/top_sp_dim;<span class="comment">//这个批次中第几个通道（考虑步长）                     </span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> c_in = c_out;<span class="comment">//这个批次中第几个通道（考虑步长）                              </span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> sp_idx = idx%top_sp_dim; <span class="comment">//通道里第几个数据（考虑步长）                </span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> h = sp_idx/top_width;<span class="comment">//这个通道中第几帧               </span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> w = sp_idx%top_width;<span class="comment">//这帧中第几个节点            </span></span><br><span class="line">        <span class="type">const</span> <span class="type">scalar_t</span>* data_im_ptr = input + n*channel*bottom_sp_dim + c_in*bottom_sp_dim; <span class="comment">//使得指针指向当前通道的第一个数据的位置，</span></span><br><span class="line">        <span class="comment">//因为传入的数据在调用这个核函数时转换成了一维的数据所以可以这样处理。</span></span><br><span class="line"></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> h_offset = h * stride;<span class="comment">//当前为第几帧</span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> w_offset = w;<span class="comment">// 当前为帧中第几个节点</span></span><br><span class="line"></span><br><span class="line">        <span class="type">scalar_t</span> val = <span class="number">0</span>;</span><br><span class="line">        <span class="type">const</span> <span class="type">scalar_t</span> x = xpos[c_in];<span class="comment">//该通道的骨架维度的可偏移参数，未训练时的初始值为0</span></span><br><span class="line">        <span class="type">const</span> <span class="type">scalar_t</span> y = ypos[c_in];<span class="comment">//该通道的时间维度的可偏移参数，未训练时的初始值为0</span></span><br><span class="line">        <span class="comment">//假设骨架维度的偏移值为u+du，时间维度的偏移值为w+dw，du为偏移值的小数部分，dw同理</span></span><br><span class="line">        <span class="type">int</span> h_im, w_im;</span><br><span class="line">        <span class="type">int</span> x1 = <span class="built_in">floorf</span>(x);<span class="comment">//向下取整u</span></span><br><span class="line">        <span class="type">int</span> x2 = x1+<span class="number">1</span>;<span class="comment">//u+1</span></span><br><span class="line">        <span class="type">int</span> y1 = <span class="built_in">floorf</span>(y);<span class="comment">//向下取整w</span></span><br><span class="line">        <span class="type">int</span> y2 = y1+<span class="number">1</span>;<span class="comment">//w+1</span></span><br><span class="line"></span><br><span class="line">        h_im = h_offset + y1;<span class="comment">//当前帧偏移w的位置</span></span><br><span class="line">        w_im = w_offset + x1;<span class="comment">//当前骨架的位置偏移u的位置</span></span><br><span class="line">        <span class="type">scalar_t</span> q11 = (h_im &gt;= <span class="number">0</span> &amp;&amp; w_im &gt;= <span class="number">0</span> &amp;&amp; h_im &lt; bottom_height &amp;&amp; w_im &lt; bottom_width) ? data_im_ptr[h_im*bottom_width + w_im] : <span class="number">0</span>;</span><br><span class="line">        <span class="comment">//如果偏移的位置不超过当前通道边界则取该值，如果超过边界则取0，下边同理</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        h_im = h_offset + y1;<span class="comment">//当前帧偏移w的位置</span></span><br><span class="line">        w_im = w_offset + x2;<span class="comment">//当前骨架的位置偏移u+1的位置</span></span><br><span class="line">        <span class="type">scalar_t</span> q21 = (h_im &gt;= <span class="number">0</span> &amp;&amp; w_im &gt;= <span class="number">0</span> &amp;&amp; h_im &lt; bottom_height &amp;&amp; w_im &lt; bottom_width) ? data_im_ptr[h_im*bottom_width + w_im] : <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        h_im = h_offset + y2;<span class="comment">//当前帧偏移w+1的位置</span></span><br><span class="line">        w_im = w_offset + x1;<span class="comment">//当前骨架的位置偏移u的位置</span></span><br><span class="line">        <span class="type">scalar_t</span> q12 = (h_im &gt;= <span class="number">0</span> &amp;&amp; w_im &gt;= <span class="number">0</span> &amp;&amp; h_im &lt; bottom_height &amp;&amp; w_im &lt; bottom_width) ? data_im_ptr[h_im*bottom_width + w_im] : <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        h_im = h_offset + y2;<span class="comment">//当前帧偏移w+1的位置</span></span><br><span class="line">        w_im = w_offset + x2;<span class="comment">//当前骨架的位置偏移u+1的位置</span></span><br><span class="line">        <span class="type">scalar_t</span> q22 = (h_im &gt;= <span class="number">0</span> &amp;&amp; w_im &gt;= <span class="number">0</span> &amp;&amp; h_im &lt; bottom_height &amp;&amp; w_im &lt; bottom_width) ? data_im_ptr[h_im*bottom_width + w_im] : <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="type">scalar_t</span> dx = x-x1;<span class="comment">//du，骨架维度的偏移值的小数部分</span></span><br><span class="line">        <span class="type">scalar_t</span> dy = y-y1;<span class="comment">//dw，时间维度的偏移值的小数部分</span></span><br><span class="line"></span><br><span class="line">        val = q11*(<span class="number">1</span>-dx)*(<span class="number">1</span>-dy) + q21*dx*(<span class="number">1</span>-dy) + q12*(<span class="number">1</span>-dx)*dy + q22*dx*dy;</span><br><span class="line">        <span class="comment">//用两个可偏移参数小数作为权重，权重之和为1</span></span><br><span class="line">        output[index] = val;<span class="comment">//保存在out数组中</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span><br><span class="line">  <span class="function">__global__ <span class="type">void</span> <span class="title">Shift_Bottom_Backward_Stride1</span><span class="params">(<span class="comment">//通过负的偏移值将梯度传递给上一层(反向shift操作)，大抵相当于将卷积中180度反转，将下一层的梯度乘以权重后传递给上一层，这个函数是不考虑步长的，步长默认为1</span></span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">scalar_t</span>* __restrict__ grad_output,<span class="comment">//每个不同进程处理不同的梯度（下一层传过来的）</span></span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span>* grad_input,<span class="comment">//用于返回给上一层的梯度值</span></span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span>* xpos,<span class="comment">//ctx保存的骨架维度的偏移值</span></span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span>* ypos,<span class="comment">//ctx保存的时间维度的偏移值</span></span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">int</span> batch,<span class="comment">//批次数</span></span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">int</span> channel,<span class="comment">//通道数</span></span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">int</span> bottom_height,<span class="comment">//同一通道的帧数</span></span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">int</span> bottom_width)</span>  <span class="comment">//同一帧的骨架节点数</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">      <span class="type">const</span> <span class="type">int</span> index = blockIdx.x * blockDim.x + threadIdx.x;<span class="comment">//和上一个核函数一样用于计算本线程是第几个线程</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (index &lt; batch*channel*bottom_height*bottom_width)</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="type">const</span> <span class="type">int</span> top_sp_dim = bottom_height * bottom_width;<span class="comment">//帧数乘以骨架的节点数，为通道的总数据大小</span></span><br><span class="line">          <span class="type">const</span> <span class="type">int</span> bottom_sp_dim = bottom_height * bottom_width;<span class="comment">//和上边相等   </span></span><br><span class="line">          <span class="type">const</span> <span class="type">int</span> n = index/(channel * bottom_sp_dim);<span class="comment">//第几个批次</span></span><br><span class="line">          <span class="type">const</span> <span class="type">int</span> idx = index%(channel * bottom_sp_dim);<span class="comment">//当前批次中第几个数据</span></span><br><span class="line">          <span class="type">const</span> <span class="type">int</span> c_in = idx/bottom_sp_dim;<span class="comment">//当前批次中第几个通道</span></span><br><span class="line">          <span class="type">const</span> <span class="type">int</span> c_out = c_in;</span><br><span class="line">          <span class="type">const</span> <span class="type">int</span> sp_idx = idx%bottom_sp_dim;<span class="comment">//通道中第几个数据</span></span><br><span class="line">          <span class="type">const</span> <span class="type">int</span> h_col = sp_idx/bottom_width;<span class="comment">//当前通道中第几帧</span></span><br><span class="line">          <span class="type">const</span> <span class="type">int</span> w_col = sp_idx%bottom_width;<span class="comment">//当前帧中第几个数据</span></span><br><span class="line">          <span class="type">const</span> <span class="type">scalar_t</span>* top_diff_ptr = grad_output + n*channel*top_sp_dim + c_out*top_sp_dim;。<span class="comment">//将下一层传入进来的梯度的指针指向当前维度的第一个数据</span></span><br><span class="line"></span><br><span class="line">          <span class="type">const</span> <span class="type">int</span> h_offset = h_col;</span><br><span class="line">          <span class="type">const</span> <span class="type">int</span> w_offset = w_col;</span><br><span class="line"></span><br><span class="line">          <span class="type">scalar_t</span> val = <span class="number">0</span>;</span><br><span class="line">          <span class="type">const</span> <span class="type">scalar_t</span> x = -xpos[c_in];<span class="comment">//反向的偏移值</span></span><br><span class="line">          <span class="type">const</span> <span class="type">scalar_t</span> y = -ypos[c_in];<span class="comment">//反向的偏移值</span></span><br><span class="line"></span><br><span class="line">          <span class="type">int</span> h_im, w_im;</span><br><span class="line"></span><br><span class="line">          <span class="type">int</span> x1 = <span class="built_in">floorf</span>(x);<span class="comment">//-u</span></span><br><span class="line">          <span class="type">int</span> x2 = x1+<span class="number">1</span>;<span class="comment">//-(u-1)</span></span><br><span class="line">          <span class="type">int</span> y1 = <span class="built_in">floorf</span>(y);<span class="comment">//-w</span></span><br><span class="line">          <span class="type">int</span> y2 = y1+<span class="number">1</span>;<span class="comment">//-(w-1)</span></span><br><span class="line"></span><br><span class="line">          <span class="comment">//q11</span></span><br><span class="line">          <span class="type">scalar_t</span> q11 = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">          h_im = (h_offset + y1);</span><br><span class="line">          w_im = (w_offset + x1);</span><br><span class="line">          q11 = (h_im &gt;= <span class="number">0</span> &amp;&amp; w_im &gt;= <span class="number">0</span> &amp;&amp; h_im &lt; bottom_height &amp;&amp; w_im &lt; bottom_width) ? top_diff_ptr[h_im*bottom_width + w_im] : <span class="number">0</span>;<span class="comment">//取相对(-w,-u)的梯度值</span></span><br><span class="line"></span><br><span class="line">          <span class="comment">//q21</span></span><br><span class="line">          <span class="type">scalar_t</span> q21 = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">          h_im = (h_offset + y1);</span><br><span class="line">          w_im = (w_offset + x2);</span><br><span class="line">          q21 = (h_im &gt;= <span class="number">0</span> &amp;&amp; w_im &gt;= <span class="number">0</span> &amp;&amp; h_im &lt; bottom_height &amp;&amp; w_im &lt; bottom_width) ? top_diff_ptr[h_im*bottom_width + w_im] : <span class="number">0</span>;<span class="comment">//取相对(-w,-u+1)的梯度值</span></span><br><span class="line"></span><br><span class="line">          <span class="comment">//q12</span></span><br><span class="line">          <span class="type">scalar_t</span> q12 = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">          h_im = (h_offset + y2);</span><br><span class="line">          w_im = (w_offset + x1);</span><br><span class="line">          q12 = (h_im &gt;= <span class="number">0</span> &amp;&amp; w_im &gt;= <span class="number">0</span> &amp;&amp; h_im &lt; bottom_height &amp;&amp; w_im &lt; bottom_width) ? top_diff_ptr[h_im*bottom_width + w_im] : <span class="number">0</span>;<span class="comment">//取相对(-w+1,-u)的梯度值</span></span><br><span class="line"></span><br><span class="line">          <span class="comment">//q22</span></span><br><span class="line">          <span class="type">scalar_t</span> q22 = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">          h_im = (h_offset + y2);</span><br><span class="line">          w_im = (w_offset + x2);</span><br><span class="line">          q22 = (h_im &gt;= <span class="number">0</span> &amp;&amp; w_im &gt;= <span class="number">0</span> &amp;&amp; h_im &lt; bottom_height &amp;&amp; w_im &lt; bottom_width) ? top_diff_ptr[h_im*bottom_width + w_im] : <span class="number">0</span>;<span class="comment">//取相对(-w+1,-u+1)的梯度值</span></span><br><span class="line"></span><br><span class="line">          <span class="type">scalar_t</span> dx = x-x1;<span class="comment">//同样求小数部分作为权重</span></span><br><span class="line">          <span class="type">scalar_t</span> dy = y-y1;</span><br><span class="line"></span><br><span class="line">          val = q11*(<span class="number">1</span>-dx)*(<span class="number">1</span>-dy) + q21*dx*(<span class="number">1</span>-dy) + q12*(<span class="number">1</span>-dx)*dy + q22*dx*dy;</span><br><span class="line">          <span class="comment">//因为通道相同的的话，他的偏移参数相同，所以就像卷积核移动一样可以遍历所有的点，</span></span><br><span class="line">          <span class="comment">//将下一层的梯度值乘以对应的权重，然后返回给上一层。</span></span><br><span class="line">          grad_input[index] = val;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span><br><span class="line">  <span class="function">__global__ <span class="type">void</span> <span class="title">Shift_Bottom_Backward</span><span class="params">(<span class="comment">//这个函数与上面的函数作用是一样的只是这个考虑步长，所以就不做解释了</span></span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">scalar_t</span>* __restrict__ grad_output,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span>* grad_input,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span>* xpos,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span>* ypos,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">int</span> batch,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">int</span> channel,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">int</span> bottom_height,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">int</span> bottom_width)</span>  </span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> index = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (index &lt; batch*channel*bottom_height*bottom_width)</span><br><span class="line">    &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> top_height = bottom_height/<span class="number">2</span>;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> top_width = bottom_width;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> stride = <span class="number">2</span>;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> top_sp_dim = top_height * top_width;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> bottom_sp_dim = bottom_height * bottom_width;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> n = index/(channel * bottom_sp_dim);</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> idx = index%(channel * bottom_sp_dim);</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> c_in = idx/bottom_sp_dim;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> c_out = c_in;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> sp_idx = idx%bottom_sp_dim;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> h_col = sp_idx/bottom_width;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> w_col = sp_idx%bottom_width;</span><br><span class="line">        <span class="type">const</span> <span class="type">scalar_t</span>* top_diff_ptr = grad_output + n*channel*top_sp_dim + c_out*top_sp_dim;</span><br><span class="line"></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> h_offset = h_col;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> w_offset = w_col;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="type">scalar_t</span> val = <span class="number">0</span>;</span><br><span class="line">        <span class="type">const</span> <span class="type">scalar_t</span> x = -xpos[c_in]; </span><br><span class="line">        <span class="type">const</span> <span class="type">scalar_t</span> y = -ypos[c_in];</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> h_im, w_im;</span><br><span class="line">        <span class="type">int</span> x1 = <span class="built_in">floorf</span>(x);</span><br><span class="line">        <span class="type">int</span> x2 = x1+<span class="number">1</span>;</span><br><span class="line">        <span class="type">int</span> y1 = <span class="built_in">floorf</span>(y);</span><br><span class="line">        <span class="type">int</span> y2 = y1+<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//q11</span></span><br><span class="line">        <span class="type">scalar_t</span> q11 = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        h_im = (h_offset + y1);</span><br><span class="line">        w_im = (w_offset + x1);</span><br><span class="line">        <span class="keyword">if</span>(h_im%stride == <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">          h_im=h_im/stride;</span><br><span class="line"></span><br><span class="line">          q11 = (h_im &gt;= <span class="number">0</span> &amp;&amp; w_im &gt;= <span class="number">0</span> &amp;&amp; h_im &lt; top_height &amp;&amp; w_im &lt; top_width) ? top_diff_ptr[h_im*top_width + w_im] : <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//q21</span></span><br><span class="line">        <span class="type">scalar_t</span> q21 = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        h_im = (h_offset + y1);</span><br><span class="line">        w_im = (w_offset + x2);</span><br><span class="line">        <span class="keyword">if</span>(h_im%stride == <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">          h_im=h_im/stride;</span><br><span class="line"></span><br><span class="line">          q21 = (h_im &gt;= <span class="number">0</span> &amp;&amp; w_im &gt;= <span class="number">0</span> &amp;&amp; h_im &lt; top_height &amp;&amp; w_im &lt; top_width) ? top_diff_ptr[h_im*top_width + w_im] : <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//q12</span></span><br><span class="line">        <span class="type">scalar_t</span> q12 = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        h_im = (h_offset + y2);</span><br><span class="line">        w_im = (w_offset + x1);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(h_im%stride == <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">          h_im=h_im/stride;</span><br><span class="line"></span><br><span class="line">          q12 = (h_im &gt;= <span class="number">0</span> &amp;&amp; w_im &gt;= <span class="number">0</span> &amp;&amp; h_im &lt; top_height &amp;&amp; w_im &lt; top_width) ? top_diff_ptr[h_im*top_width + w_im] : <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//q22</span></span><br><span class="line">        <span class="type">scalar_t</span> q22 = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        h_im = (h_offset + y2);</span><br><span class="line">        w_im = (w_offset + x2);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(h_im%stride == <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">          h_im=h_im/stride;</span><br><span class="line"></span><br><span class="line">          q22 = (h_im &gt;= <span class="number">0</span> &amp;&amp; w_im &gt;= <span class="number">0</span> &amp;&amp; h_im &lt; top_height &amp;&amp; w_im &lt; top_width) ? top_diff_ptr[h_im*top_width + w_im] : <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">scalar_t</span> dx = x-x1;</span><br><span class="line">        <span class="type">scalar_t</span> dy = y-y1;</span><br><span class="line"></span><br><span class="line">        val = q11*(<span class="number">1</span>-dx)*(<span class="number">1</span>-dy) + q21*dx*(<span class="number">1</span>-dy) + q12*(<span class="number">1</span>-dx)*dy + q22*dx*dy;</span><br><span class="line">        grad_input[index] = val;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="comment">// namespace</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;<span class="comment">//下面这三个函数用于原子加法，在代码中没有被调用</span></span><br><span class="line">  <span class="function">__inline__ __device__ <span class="type">void</span> <span class="title">myAtomicAdd</span><span class="params">(<span class="type">scalar_t</span> *buf, <span class="type">scalar_t</span> val)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;&gt;</span><br><span class="line">  __inline__ __device__ <span class="type">void</span> <span class="built_in">myAtomicAdd</span>&lt;<span class="type">float</span>&gt;(<span class="type">float</span> *buf, <span class="type">float</span> val)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="built_in">atomicAdd</span>(buf, val);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;&gt;</span><br><span class="line">  __inline__ __device__ <span class="type">void</span> <span class="built_in">myAtomicAdd</span>&lt;<span class="type">double</span>&gt;(<span class="type">double</span> *buf, <span class="type">double</span> val)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="comment">//Not Supported</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span><br><span class="line">  <span class="function">__global__ <span class="type">void</span> <span class="title">Shift_Position_Backward</span><span class="params">(<span class="comment">//上面的两个函数都是用于求返回给上一级的梯度值，这个函数是求偏移值的梯度</span></span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">scalar_t</span>* __restrict__ input,<span class="comment">//正向传递的输入值</span></span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">scalar_t</span>* __restrict__ grad_output,<span class="comment">//下一层传过来的梯度值</span></span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span>* grad_input,<span class="comment">//返回给上一层的梯度值</span></span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span>* xpos,<span class="comment">//骨架维度的偏移值</span></span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span>* ypos,<span class="comment">//时间维度的偏移值</span></span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span>* grad_xpos_bchw,<span class="comment">//骨架维度偏移值的梯度值</span></span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span>* grad_ypos_bchw,<span class="comment">//时间维度的偏移值的梯度值</span></span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">int</span> batch,<span class="comment">//批次数</span></span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">int</span> channel,<span class="comment">//通道数</span></span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">int</span> bottom_height,<span class="comment">//帧数</span></span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">int</span> bottom_width,<span class="comment">//骨架点数</span></span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">int</span> stride)</span>  <span class="comment">//步长</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> index = blockIdx.x * blockDim.x + threadIdx.x;<span class="comment">//计算是第几个进程</span></span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> top_height = bottom_height/stride;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> top_width = bottom_width;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (index &lt; batch*channel*top_height*top_width)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> top_sp_dim = top_height * top_width;<span class="comment">//一个通道的总数据数（考虑步长）</span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> bottom_sp_dim = bottom_height * bottom_width;<span class="comment">//一个通道的总数据数</span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> n = index/(channel * top_sp_dim);<span class="comment">//第几个批次（考虑步长）</span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> idx = index%(channel * top_sp_dim);<span class="comment">//这个批次内第几个数据（考虑步长）</span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> c_mul = <span class="number">1</span>;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> c_out = idx/top_sp_dim;<span class="comment">//当前批次的第几个通道（考虑步长）</span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> c_in = c_out/c_mul;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> sp_idx = idx%top_sp_dim;<span class="comment">//当前通道的第几个数据（考虑步长）</span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> h = sp_idx/top_width;<span class="comment">//当前通道的第几帧（考虑步长）</span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> w = sp_idx%top_width;<span class="comment">//当前帧内第几个节点（考虑步长）</span></span><br><span class="line">        <span class="type">const</span> <span class="type">scalar_t</span>* data_im_ptr = input + n*channel*bottom_sp_dim + c_in*bottom_sp_dim;<span class="comment">//定位到当前通道的第一个数据</span></span><br><span class="line"></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> h_offset = h * stride;<span class="comment">//当前第几帧</span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> w_offset = w;<span class="comment">//当前是第几个节点</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//output : 2*(C) x (1*H*W)</span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> kernel_offset = top_sp_dim;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> c_off = c_out % c_mul;</span><br><span class="line"></span><br><span class="line">        <span class="type">scalar_t</span> val_x = <span class="number">0</span>, val_y = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="type">const</span> <span class="type">scalar_t</span> shiftX = xpos[c_in];<span class="comment">//当前通道的骨架维度的偏移值</span></span><br><span class="line">        <span class="type">const</span> <span class="type">scalar_t</span> shiftY = ypos[c_in];<span class="comment">//当前通道的时间维度的偏移值</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> ix1 = <span class="built_in">floorf</span>(shiftX);<span class="comment">//向下取整，u</span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> ix2 = ix1+<span class="number">1</span>;<span class="comment">//u+1</span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> iy1 = <span class="built_in">floorf</span>(shiftY);<span class="comment">//w</span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> iy2 = iy1+<span class="number">1</span>;<span class="comment">//w+1</span></span><br><span class="line">        <span class="type">const</span> <span class="type">scalar_t</span> dx = shiftX-ix1;<span class="comment">//du，骨架维度偏移值的小数部分</span></span><br><span class="line">        <span class="type">const</span> <span class="type">scalar_t</span> dy = shiftY-iy1;<span class="comment">//dw，时间偏移值的小数部分</span></span><br><span class="line"></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> h_im1 = h_offset + iy1;<span class="comment">//u</span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> h_im2 = h_offset + iy2;<span class="comment">//u+1</span></span><br><span class="line"></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> w_im1 = w_offset + ix1;<span class="comment">//w</span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> w_im2 = w_offset + ix2;<span class="comment">//W+1</span></span><br><span class="line"></span><br><span class="line">        <span class="type">const</span> <span class="type">scalar_t</span> q11 = (h_im1 &gt;= <span class="number">0</span> &amp;&amp; w_im1 &gt;= <span class="number">0</span> &amp;&amp; h_im1 &lt; bottom_height &amp;&amp; w_im1 &lt; bottom_width) ? data_im_ptr[h_im1*bottom_width + w_im1] : <span class="number">0</span>;</span><br><span class="line">        <span class="type">const</span> <span class="type">scalar_t</span> q21 = (h_im1 &gt;= <span class="number">0</span> &amp;&amp; w_im2 &gt;= <span class="number">0</span> &amp;&amp; h_im1 &lt; bottom_height &amp;&amp; w_im2 &lt; bottom_width) ? data_im_ptr[h_im1*bottom_width + w_im2] : <span class="number">0</span>;</span><br><span class="line">        <span class="type">const</span> <span class="type">scalar_t</span> q12 = (h_im2 &gt;= <span class="number">0</span> &amp;&amp; w_im1 &gt;= <span class="number">0</span> &amp;&amp; h_im2 &lt; bottom_height &amp;&amp; w_im1 &lt; bottom_width) ? data_im_ptr[h_im2*bottom_width + w_im1] : <span class="number">0</span>;</span><br><span class="line">        <span class="type">const</span> <span class="type">scalar_t</span> q22 = (h_im2 &gt;= <span class="number">0</span> &amp;&amp; w_im2 &gt;= <span class="number">0</span> &amp;&amp; h_im2 &lt; bottom_height &amp;&amp; w_im2 &lt; bottom_width) ? data_im_ptr[h_im2*bottom_width + w_im2] : <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        val_x = (<span class="number">1</span>-dy)*(q21-q11)+dy*(q22-q12);<span class="comment">//在原来计算公式中对du（骨架维度偏移值的小数部分）求导得到的公式，推导过程下面有图展示，这里x与y都是标量</span></span><br><span class="line">        val_y = (<span class="number">1</span>-dx)*(q12-q11)+dx*(q22-q21);<span class="comment">//在原来计算公式中对dw（时间维度偏移值的小数部分）求导分得到的公式</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       grad_xpos_bchw[index] = val_x * grad_output[index];</span><br><span class="line">        <span class="comment">//根据链式法则将计算结果对du求导得到值相乘得到损失函数对du的，骨架维度的梯度值放入到对应的数组内</span></span><br><span class="line">        grad_ypos_bchw[index] = val_y * grad_output[index];</span><br><span class="line">        <span class="comment">//时间维度的梯度值放入到对应的数组内</span></span><br><span class="line">        <span class="comment">//可能有的同学想问整数部分不需要求梯度值吗，这里做解释因为整数部分做了取整的操作求不了导所以不能求梯度</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="comment">// namespace</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> <span class="type">scalar_t</span>&gt;</span><br><span class="line">  <span class="function">__global__ <span class="type">void</span> <span class="title">applyShiftConstraint</span><span class="params">(<span class="comment">//约束偏移值的梯度变化</span></span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span>* grad_xpos,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">scalar_t</span>* grad_ypos,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> <span class="type">int</span> channel)</span>  </span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> index = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (index &lt; channel)</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="type">const</span> <span class="type">scalar_t</span> dx = grad_xpos[index];</span><br><span class="line">      <span class="type">const</span> <span class="type">scalar_t</span> dy = grad_ypos[index];</span><br><span class="line">      <span class="type">const</span> <span class="type">scalar_t</span> dr = <span class="built_in">sqrt</span>(dy*dy);</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span>(dr!=<span class="number">0</span>)</span><br><span class="line">      &#123;</span><br><span class="line">         grad_xpos[index] = dx/dr*<span class="number">0.0</span>;</span><br><span class="line">         grad_ypos[index] = dy/dr*<span class="number">0.01</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">else</span>                                  <span class="comment">// without this, the grad_ypos may be large.</span></span><br><span class="line">      &#123;</span><br><span class="line">         grad_xpos[index] = <span class="number">0.0</span>;</span><br><span class="line">         grad_ypos[index] = <span class="number">0.0001</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="comment">// namespace</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function">at::Tensor <span class="title">shift_cuda_forward</span><span class="params">(<span class="comment">//shift的forward操作，并行调用shift_cuda_forward_kernel核函数实现</span></span></span></span><br><span class="line"><span class="params"><span class="function">    at::Tensor input,at::Tensor xpos,at::Tensor ypos,<span class="type">const</span> <span class="type">int</span> stride)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> output = at::<span class="built_in">zeros</span>(&#123;input.<span class="built_in">size</span>(<span class="number">0</span>), input.<span class="built_in">size</span>(<span class="number">1</span>), input.<span class="built_in">size</span>(<span class="number">2</span>)/stride, input.<span class="built_in">size</span>(<span class="number">3</span>)&#125;, input.<span class="built_in">options</span>());<span class="comment">//初始化结果张量</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">const</span> dim3 <span class="title">blocks</span><span class="params">((input.size(<span class="number">0</span>)*input.size(<span class="number">1</span>)*input.size(<span class="number">2</span>)*input.size(<span class="number">3</span>)/stride+<span class="number">1024</span><span class="number">-1</span>)/<span class="number">1024</span>)</span></span>;<span class="comment">//计算需要几个块（一维）</span></span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> threads = <span class="number">1024</span>;<span class="comment">//定义一个块有几个线程（一维）</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">AT_DISPATCH_FLOATING_TYPES</span>(input.<span class="built_in">type</span>(), <span class="string">&quot;shift_forward_cuda&quot;</span>, ([&amp;] &#123;</span><br><span class="line">    shift_cuda_forward_kernel&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(</span><br><span class="line">      input.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">      output.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">      xpos.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">      ypos.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">      input.<span class="built_in">size</span>(<span class="number">0</span>),</span><br><span class="line">      input.<span class="built_in">size</span>(<span class="number">1</span>),</span><br><span class="line">      input.<span class="built_in">size</span>(<span class="number">2</span>),</span><br><span class="line">      input.<span class="built_in">size</span>(<span class="number">3</span>),</span><br><span class="line">      input.<span class="built_in">size</span>(<span class="number">2</span>)/stride,</span><br><span class="line">      input.<span class="built_in">size</span>(<span class="number">3</span>),</span><br><span class="line">      stride);</span><br><span class="line">  &#125;));</span><br><span class="line">    <span class="comment">//AT_DISPATCH_FLOATING_TYPES是一个封装的接口，可以替换成AT_DISPATCH_ALL_TYPES。</span></span><br><span class="line">    <span class="comment">//它有三个参数，第一个是tensor的数据类型，第二个是用于显示错误的信息，第三个是个匿名函数，</span></span><br><span class="line">    <span class="comment">//([&amp;]&#123; &#125;)内写cuda的__global__ kernel函数。</span></span><br><span class="line">	<span class="comment">//input.data&lt;scalar_t&gt;() 把input的数据转换成scalar_t类型并且返回一个头指针，</span></span><br><span class="line">    <span class="comment">//该数据是一个一维的连续存储的地址，访问数据的方式和c语言指针使用方法一样。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> output;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">std::vector&lt;at::Tensor&gt; <span class="title">shift_cuda_backward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    at::Tensor grad_output,<span class="comment">//这是下一层传递过来的梯度值</span></span></span></span><br><span class="line"><span class="params"><span class="function">    at::Tensor input,<span class="comment">//保存的正向传播的这一层的输入</span></span></span></span><br><span class="line"><span class="params"><span class="function">    at::Tensor output,<span class="comment">//保存的正向传播的这一层的输出</span></span></span></span><br><span class="line"><span class="params"><span class="function">    at::Tensor xpos,<span class="comment">//保存的骨架维度的偏移值数组</span></span></span></span><br><span class="line"><span class="params"><span class="function">    at::Tensor ypos,<span class="comment">//保存的时间维度的偏移值数组</span></span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> stride)</span> </span>&#123;<span class="comment">//保存的步长</span></span><br><span class="line">  <span class="keyword">auto</span> grad_input = at::<span class="built_in">zeros_like</span>(input);<span class="comment">//初始化返回给上一层的梯度值</span></span><br><span class="line"><span class="comment">//shift的backward函数线先调用Shift_Bottom_Backward_Stride1_或Shift_Bottom_Backward_函数，</span></span><br><span class="line"><span class="comment">//计算返回给上一层的梯度值，再调用Shift_Position_Backward_函数计算时间维度以及骨架维度的偏移值的梯度值</span></span><br><span class="line"><span class="comment">//最后调用applyShiftConstraint_函数对偏移值梯度值的大小做约束</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">const</span> dim3 <span class="title">blocks</span><span class="params">((input.size(<span class="number">0</span>)*input.size(<span class="number">1</span>)*input.size(<span class="number">2</span>)*input.size(<span class="number">3</span>)+<span class="number">1024</span><span class="number">-1</span>)/<span class="number">1024</span>)</span></span>;<span class="comment">//计算块数（一维）</span></span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> threads = <span class="number">1024</span>;<span class="comment">//计算线程数（一维）</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span>(stride==<span class="number">1</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES</span>(input.<span class="built_in">type</span>(), <span class="string">&quot;Shift_Bottom_Backward_Stride1_&quot;</span>, ([&amp;] &#123;</span><br><span class="line">      Shift_Bottom_Backward_Stride1&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(</span><br><span class="line">        grad_output.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">        grad_input.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">        xpos.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">        ypos.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">        input.<span class="built_in">size</span>(<span class="number">0</span>),</span><br><span class="line">        input.<span class="built_in">size</span>(<span class="number">1</span>),</span><br><span class="line">        input.<span class="built_in">size</span>(<span class="number">2</span>),</span><br><span class="line">        input.<span class="built_in">size</span>(<span class="number">3</span>));</span><br><span class="line">    &#125;));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">  &#123;</span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES</span>(input.<span class="built_in">type</span>(), <span class="string">&quot;Shift_Bottom_Backward_&quot;</span>, ([&amp;] &#123;</span><br><span class="line">      Shift_Bottom_Backward&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(</span><br><span class="line">        grad_output.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">        grad_input.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">        xpos.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">        ypos.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">        input.<span class="built_in">size</span>(<span class="number">0</span>),</span><br><span class="line">        input.<span class="built_in">size</span>(<span class="number">1</span>),</span><br><span class="line">        input.<span class="built_in">size</span>(<span class="number">2</span>),</span><br><span class="line">        input.<span class="built_in">size</span>(<span class="number">3</span>));</span><br><span class="line">    &#125;));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> grad_xpos_bchw = at::<span class="built_in">zeros</span>(&#123;output.<span class="built_in">size</span>(<span class="number">0</span>), output.<span class="built_in">size</span>(<span class="number">1</span>), output.<span class="built_in">size</span>(<span class="number">2</span>), output.<span class="built_in">size</span>(<span class="number">3</span>)&#125;, output.<span class="built_in">options</span>()); <span class="comment">// (b,c,h,w)，初始化骨架维度的偏移值的梯度</span></span><br><span class="line">  <span class="keyword">auto</span> grad_ypos_bchw = at::<span class="built_in">zeros</span>(&#123;output.<span class="built_in">size</span>(<span class="number">0</span>), output.<span class="built_in">size</span>(<span class="number">1</span>), output.<span class="built_in">size</span>(<span class="number">2</span>), output.<span class="built_in">size</span>(<span class="number">3</span>)&#125;, output.<span class="built_in">options</span>()); <span class="comment">// (b,c,h,w)，初始化时间维度的偏移值的梯度</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">const</span> dim3 <span class="title">blocks_output</span><span class="params">((output.size(<span class="number">0</span>)*output.size(<span class="number">1</span>)*output.size(<span class="number">2</span>)*output.size(<span class="number">3</span>)+<span class="number">1024</span><span class="number">-1</span>)/<span class="number">1024</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">AT_DISPATCH_FLOATING_TYPES</span>(input.<span class="built_in">type</span>(), <span class="string">&quot;Shift_Position_Backward_&quot;</span>, ([&amp;] &#123;</span><br><span class="line">    Shift_Position_Backward&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;blocks_output, threads&gt;&gt;&gt;(</span><br><span class="line">      input.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">      grad_output.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">      grad_input.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">      xpos.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">      ypos.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">      grad_xpos_bchw.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">      grad_ypos_bchw.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">      input.<span class="built_in">size</span>(<span class="number">0</span>),</span><br><span class="line">      input.<span class="built_in">size</span>(<span class="number">1</span>),</span><br><span class="line">      input.<span class="built_in">size</span>(<span class="number">2</span>),</span><br><span class="line">      input.<span class="built_in">size</span>(<span class="number">3</span>),</span><br><span class="line">      stride);</span><br><span class="line">  &#125;));</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> grad_xpos_chw = at::<span class="built_in">mean</span>(grad_xpos_bchw, <span class="number">0</span>, <span class="literal">false</span>);<span class="comment">//对批次维度的梯度值求平均</span></span><br><span class="line">  <span class="keyword">auto</span> grad_xpos_ch = at::<span class="built_in">sum</span>(grad_xpos_chw, <span class="number">2</span>, <span class="literal">false</span>);<span class="comment">//对骨架维度的梯度求和</span></span><br><span class="line">  <span class="keyword">auto</span> grad_xpos_c  = at::<span class="built_in">sum</span>(grad_xpos_ch, <span class="number">1</span>, <span class="literal">false</span>);<span class="comment">//对时间维度的梯度求和</span></span><br><span class="line">  <span class="keyword">auto</span> grad_xpos = grad_xpos_c;<span class="comment">//每个通道都有一个梯度值用于每个通道的骨架的偏移值的更新</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> grad_ypos_chw = at::<span class="built_in">mean</span>(grad_ypos_bchw, <span class="number">0</span>, <span class="literal">false</span>);<span class="comment">//同理</span></span><br><span class="line">  <span class="keyword">auto</span> grad_ypos_ch = at::<span class="built_in">sum</span>(grad_ypos_chw, <span class="number">2</span>, <span class="literal">false</span>);</span><br><span class="line">  <span class="keyword">auto</span> grad_ypos_c  = at::<span class="built_in">sum</span>(grad_ypos_ch, <span class="number">1</span>, <span class="literal">false</span>);</span><br><span class="line">  <span class="keyword">auto</span> grad_ypos = grad_ypos_c;<span class="comment">//每个通道都有一个梯度用于时间维度的偏移值的更新</span></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">const</span> dim3 <span class="title">blocks_norm</span><span class="params">((output.size(<span class="number">1</span>)+<span class="number">1024</span><span class="number">-1</span>)/<span class="number">1024</span>)</span></span>;<span class="comment">//确定接下来的约束操作需要多少个块</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">AT_DISPATCH_FLOATING_TYPES</span>(input.<span class="built_in">type</span>(), <span class="string">&quot;applyShiftConstraint_&quot;</span>, ([&amp;] &#123;</span><br><span class="line">    applyShiftConstraint&lt;<span class="type">scalar_t</span>&gt;&lt;&lt;&lt;blocks_norm, threads&gt;&gt;&gt;(</span><br><span class="line">      grad_xpos.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">      grad_ypos.<span class="built_in">data</span>&lt;<span class="type">scalar_t</span>&gt;(),</span><br><span class="line">      output.<span class="built_in">size</span>(<span class="number">1</span>));</span><br><span class="line">  &#125;));</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> &#123;grad_input,grad_xpos,grad_ypos&#125;;<span class="comment">//返回，给上一层的梯度值以及骨架维度、时间维度的偏移值的梯度值</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>shift算子的foward的示意图，注意同一通道的偏移值是相同的：</p>
<p><img src="/2023/11/27/%E7%94%A8cuda%E7%BC%96%E7%A8%8B%E5%86%99shift%E7%AE%97%E5%AD%90%E5%B9%B6%E5%AF%BC%E5%85%A5python%E9%A1%B9%E7%9B%AE/image-20231129204858807.png" alt="image-20231129204858807"></p>
<p>shift算子反向传播的示意图，注意同一通道的偏移值是相同的：</p>
<p><img src="/2023/11/27/%E7%94%A8cuda%E7%BC%96%E7%A8%8B%E5%86%99shift%E7%AE%97%E5%AD%90%E5%B9%B6%E5%AF%BC%E5%85%A5python%E9%A1%B9%E7%9B%AE/image-20231129101134501.png" alt="image-20231129101134501"></p>
<p>两个偏移值计算式子的求导过程：</p>
<p><img src="/2023/11/27/%E7%94%A8cuda%E7%BC%96%E7%A8%8B%E5%86%99shift%E7%AE%97%E5%AD%90%E5%B9%B6%E5%AF%BC%E5%85%A5python%E9%A1%B9%E7%9B%AE/image-20231128202157798.png" alt="image-20231128202157798"></p>
<h2 id="shift-cuda-cpp"><a href="#shift-cuda-cpp" class="headerlink" title="shift_cuda.cpp"></a>shift_cuda.cpp</h2><p>shift_cuda.cpp编写包装函数并调用PYBIND11_MODULE对算子进行封装。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;torch/torch.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">at::Tensor <span class="title">shift_cuda_forward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    at::Tensor input,at::Tensor xpos,at::Tensor ypos,<span class="type">const</span> <span class="type">int</span> stride)</span></span>;</span><br><span class="line"><span class="comment">//声明cuda编程中写的shift算子的forward函数</span></span><br><span class="line"></span><br><span class="line"><span class="function">std::vector&lt;at::Tensor&gt; <span class="title">shift_cuda_backward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    at::Tensor grad_output,</span></span></span><br><span class="line"><span class="params"><span class="function">    at::Tensor input,</span></span></span><br><span class="line"><span class="params"><span class="function">    at::Tensor output,</span></span></span><br><span class="line"><span class="params"><span class="function">    at::Tensor xpos,</span></span></span><br><span class="line"><span class="params"><span class="function">    at::Tensor ypos,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> stride)</span></span>;</span><br><span class="line"><span class="comment">//声明cuda编程中写的shift算子的backward函数</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x <span class="string">&quot; must be a CUDA tensor&quot;</span>)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CONTIGUOUS(x) AT_ASSERTM(x.is_contiguous(), #x <span class="string">&quot; must be contiguous&quot;</span>)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)</span></span><br><span class="line"><span class="comment">//CHECK_INPUT(x) 的含义是：检查张量 x 是否为 CUDA 张量且是否为连续存储的，如果不是，则抛出异常并打印错误消息。</span></span><br><span class="line"><span class="function">at::Tensor <span class="title">shift_forward</span><span class="params">(<span class="comment">//检查输入的值并封装cuda编程中的shift_cuda_forward</span></span></span></span><br><span class="line"><span class="params"><span class="function">    at::Tensor input,at::Tensor xpos,at::Tensor ypos,<span class="type">const</span> <span class="type">int</span> stride)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">CHECK_INPUT</span>(input);</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">shift_cuda_forward</span>(input,xpos,ypos,stride);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">std::vector&lt;at::Tensor&gt; <span class="title">shift_backward</span><span class="params">(<span class="comment">//检查输入以及下一层传进来的梯度值并封装cuda编程中的shift_backward</span></span></span></span><br><span class="line"><span class="params"><span class="function">    at::Tensor grad_output,</span></span></span><br><span class="line"><span class="params"><span class="function">    at::Tensor input,</span></span></span><br><span class="line"><span class="params"><span class="function">    at::Tensor output,</span></span></span><br><span class="line"><span class="params"><span class="function">    at::Tensor xpos,</span></span></span><br><span class="line"><span class="params"><span class="function">    at::Tensor ypos,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> stride)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="built_in">CHECK_INPUT</span>(grad_output);</span><br><span class="line">  <span class="built_in">CHECK_INPUT</span>(output);</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">shift_cuda_backward</span>(</span><br><span class="line">    grad_output,</span><br><span class="line">    input,</span><br><span class="line">    output,</span><br><span class="line">    xpos,</span><br><span class="line">    ypos,</span><br><span class="line">    stride);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(TORCH_EXTENSION_NAME, m) &#123;</span><br><span class="line">    	<span class="comment">//PYBIND11_MODULE 是 Pybind11 提供的宏，用于定义 Python 扩展模块。</span></span><br><span class="line">		<span class="comment">//TORCH_EXTENSION_NAME 是扩展模块的名称，这通常在其他地方定义。</span></span><br><span class="line">  m.<span class="built_in">def</span>(<span class="string">&quot;forward&quot;</span>, &amp;shift_forward, <span class="string">&quot;shift forward (CUDA)&quot;</span>);</span><br><span class="line">  		<span class="comment">//m.def 用于在 Python 模块中定义一个函数。</span></span><br><span class="line">		<span class="comment">//&quot;forward&quot; 是 Python 中调用该函数的名称。</span></span><br><span class="line">		<span class="comment">//&amp;shift_forward 是要绑定到 Python 函数的 C++ 函数的地址。</span></span><br><span class="line">    	<span class="comment">//在这里，它绑定到名为 shift_forward 的函数。</span></span><br><span class="line">		<span class="comment">//&quot;shift forward (CUDA)&quot; 是函数的文档字符串，用于描述该函数的作用和用法。</span></span><br><span class="line">  m.<span class="built_in">def</span>(<span class="string">&quot;backward&quot;</span>, &amp;shift_backward, <span class="string">&quot;shift backward (CUDA)&quot;</span>);<span class="comment">//同理</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//这段代码使用 Pybind11 来定义一个名为 TORCH_EXTENSION_NAME 的 PyTorch 扩展模块，并注册两个函数 forward 和 backward。</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="setup-py"><a href="#setup-py" class="headerlink" title="setup.py"></a>setup.py</h2><p>新建setup.py文件配置编译信息，利用<strong>setuptools</strong>对算子打包。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> setup</span><br><span class="line"><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> BuildExtension, CUDAExtension, CppExtension</span><br><span class="line"></span><br><span class="line">setup(</span><br><span class="line">    name=<span class="string">&#x27;shift_cuda_linear_cpp&#x27;</span>,//包名</span><br><span class="line">    ext_modules=[<span class="comment">#注意这里的shift_cuda才是在python文件中导入的模块名不是上面的报名</span></span><br><span class="line">        CUDAExtension(<span class="string">&#x27;shift_cuda&#x27;</span>, [</span><br><span class="line">            <span class="string">&#x27;shift_cuda.cpp&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;shift_cuda_kernel.cu&#x27;</span>,</span><br><span class="line">        ]),<span class="comment">#其中第一个参数为对应模块的名字，第二个参数为包含所有文件路径的列表。</span></span><br><span class="line">    ],</span><br><span class="line">    cmdclass=&#123;</span><br><span class="line">        <span class="string">&#x27;build_ext&#x27;</span>: BuildExtension</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure>

<p>直接在终端运行python setup.py install就可以编译安装shift_cuda_kernel.cu、shift_cuda.cpp两个文件。</p>
<p>注意先切好需要的虚拟环境喔。</p>
<h2 id="shift-py"><a href="#shift-py" class="headerlink" title="shift.py"></a>shift.py</h2><p>为了让自定义算子能够正常正向传播、反向传播，我们需要继承torch.autograd.Function进行算子包装。</p>
<p>自定义的torch.autograd.Function类型要实现forward、backward函数，并声明为静态成员函数。</p>
<p>如果算子不需要考虑反向传播，可以用ctx.mark_non_differentiable(ans) 将函数的输出标记不需要微分。</p>
<p><strong>注意:backward的输入对应forward的输出，输出对应forward的输入</strong></p>
<p>如ShiftFunction所示</p>
<p>最后使用ShiftFunction.apply()获取最终的函数形式</p>
<p>然后使用继承了Module的shift将其ShiftFunction封装起来</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Module, Parameter</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> shift_cuda</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ShiftFunction</span>(<span class="title class_ inherited__">Function</span>):</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, <span class="built_in">input</span>,xpos,ypos,stride=<span class="number">1</span></span>):</span><br><span class="line">        <span class="keyword">if</span> stride==<span class="number">1</span>:</span><br><span class="line">            xpos = xpos</span><br><span class="line">            ypos = ypos</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            ypos = ypos + <span class="number">0.5</span></span><br><span class="line">            <span class="comment"># ypos = ypos + 0.5</span></span><br><span class="line">        output = shift_cuda.forward(<span class="built_in">input</span>,xpos,ypos,stride)</span><br><span class="line">        ctx.save_for_backward(<span class="built_in">input</span>, output, xpos, ypos)</span><br><span class="line">        ctx.stride = stride</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_output</span>): </span><br><span class="line">        grad_output = grad_output.contiguous()</span><br><span class="line">        <span class="built_in">input</span>, output, xpos, ypos = ctx.saved_variables</span><br><span class="line">        grad_input,grad_xpos,grad_ypos = shift_cuda.backward(grad_output, <span class="built_in">input</span>, output, xpos, ypos, ctx.stride)</span><br><span class="line">        <span class="keyword">return</span> grad_input, grad_xpos, grad_ypos, <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Shift</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channel, stride, init_scale=<span class="number">3</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Shift, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">        self.xpos = Parameter(torch.zeros(channel,requires_grad=<span class="literal">True</span>,device=<span class="string">&#x27;cuda&#x27;</span>)*<span class="number">1.5</span>)</span><br><span class="line">        self.ypos = Parameter(torch.zeros(channel,requires_grad=<span class="literal">True</span>,device=<span class="string">&#x27;cuda&#x27;</span>)*<span class="number">1.5</span>)</span><br><span class="line"></span><br><span class="line">        self.xpos.data.uniform_(-<span class="number">1e-8</span>,<span class="number">1e-8</span>)</span><br><span class="line">        self.ypos.data.uniform_(-init_scale,init_scale)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="keyword">return</span> ShiftFunction.apply(<span class="built_in">input</span>,self.xpos,self.ypos,self.stride)</span><br></pre></td></tr></table></figure>


            </div>

            <!-- Post Comments -->
            
    <!-- 使用 DISQUS_CLICK -->
<div id="disqus-comment">
    <div id="disqus_thread"></div>

<!-- add animation -->
<style>
	.disqus_click_btn {
            line-height: 30px;
            margin: 0;
            min-width: 50px;
            padding: 0 14px;
            display: inline-block;
            font-family: "Roboto", "Helvetica", "Arial", sans-serif;
            font-size: 14px;
            font-weight: 400;
            text-transform: uppercase;
            letter-spacing: 0;
            overflow: hidden;
            will-change: box-shadow;
            transition: box-shadow .2s cubic-bezier(.4, 0, 1, 1), background-color .2s cubic-bezier(.4, 0, .2, 1), color .2s cubic-bezier(.4, 0, .2, 1);
            outline: 0;
            cursor: pointer;
            text-decoration: none;
            text-align: center;
            vertical-align: middle;
            border: 0;
            background: rgba(158, 158, 158, .2);
            box-shadow: 0 2px 2px 0 rgba(0, 0, 0, .14), 0 3px 1px -2px rgba(0, 0, 0, .2), 0 1px 5px 0 rgba(0, 0, 0, .12);
            color: #fff;
            background-color: #7EC0EE;
            text-shadow: 0
        }
</style>
	
<div class="btn_click_load" id="disqus_bt"> 
    <button class="disqus_click_btn">点击查看评论</button>
</div>

<!--
<script type="text/javascript">
$('.btn_click_load').click(function() {
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'http-miccall-tech'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    document.getElementById('disqus_bt').style.display = "none";
});
</script>
-->
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'https://blog.ldf.icu/2023/11/27/%E7%94%A8cuda%E7%BC%96%E7%A8%8B%E5%86%99shift%E7%AE%97%E5%AD%90%E5%B9%B6%E5%AF%BC%E5%85%A5python%E9%A1%B9%E7%9B%AE/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'https://blog.ldf.icu/2023/11/27/%E7%94%A8cuda%E7%BC%96%E7%A8%8B%E5%86%99shift%E7%AE%97%E5%AD%90%E5%B9%B6%E5%AF%BC%E5%85%A5python%E9%A1%B9%E7%9B%AE/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>

<script type="text/javascript">
    $('.btn_click_load').click(function() {  //click to load comments
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document;
            var s = d.createElement('script');
            s.src = '//http-miccall-tech.disqus.com/embed.js';
            s.setAttribute('data-timestamp', + new Date());
            (d.head || d.body).appendChild(s);
        })();
        $('.btn_click_load').css('display','none');
    });
</script>
</div>
<style>
    #disqus-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Design: <a target="_blank" rel="noopener" href="http://miccall.tech " style="border-bottom: none;">miccall</a></li>
            </ul>
            
                <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span>
			
        </div>
    </div>
</body>



 	
</html>
