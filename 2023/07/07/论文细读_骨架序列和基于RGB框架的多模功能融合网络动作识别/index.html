<!DOCTYPE HTML>
<html>

<head>
	<link rel="bookmark"  type="image/x-icon"  href="/img/logo_miccall.png"/>
	<link rel="shortcut icon" href="/img/logo_miccall.png">
	
			    <title>
    锋哥的个人网站
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
    <meta name="keywords" content="LDF" />
    
    	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	 
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/bg.jpg') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
<meta name="generator" content="Hexo 6.3.0"></head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css" />
<link rel="stylesheet" href="/css/typo.css" />
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">LDF</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">主页</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1">分类</a>
	                    <ul class="submenu">
	                        <li>
	                        
	                    </ul>
	        </li>
	        
	        <!-- archives  归档   --> 
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/about/" title="简历">
		                简历
		            </a>
		        </li>
		        
		        <li>
		            <a href="/group/" title="团队">
		                团队
		            </a>
		        </li>
		        
		        <li>
		            <a href="/gallery/" title="图库">
		                图库
		            </a>
		        </li>
		        
		        <li>
		            <a href="/tag/" title="标签">
		                标签
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/ldfgood" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
                    <li>
                        <a title="csdn" href="https://blog.csdn.net/weixin_64089712" target="_blank" rel="noopener">
                            <i class="icon fa fa-csdn"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img" style="height: 25rem;background-image: url();background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2 >论文细读_Skeleton Sequence and RGB Frame Based Multi-Modality Feature Fusion Network for Action Recognition</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <h1 id="个人总结"><a href="#个人总结" class="headerlink" title="个人总结"></a>个人总结</h1><p>该论文提出一种框架基于骨架序列与RGB帧的特征融合框架。</p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230708212822409-1688822905187-1.png" alt="image-20230708212822409"></p>
<p><strong>骨架注意力模块</strong>:使用attention mask注意于人移动最大的关节的位置用来提取人与物的交互信息。</p>
<p>**自注意力模块:**用作抑制背景信息与强调人类特性。</p>
<p>后期融合模块：</p>
<p>由于Bi-LSTM与的特征图维度不同所以需要不同使用不同的方式融合RGB流与骨架序列。</p>
<p>​		这里只看ST-GCN因为已经使用骨架注意力模块以及自注意力模块帮助RGB流提取关键信息了，所以我们只需要专注于融合空间信息。</p>
<p>首先将RGB特征中的两个注意力特征相加后求平均值得到RGB特征后使用最大池化层将RGB特征与骨架特征的每个通道压缩到只有一个值。</p>
<p>然后对RGB特征与骨架特征进行组合，形成组合特征如图所示。</p>
<img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/clip_image002-1689749528073-8.png" alt="img" style="zoom:50%;">

<p>然后输入下图网络，通过两个一维卷积以及softmax函数探索跨空间关系并生成关系掩码然后按逐元素乘以组合特征，然后输入一个GAP层和两个全连接层，然后是一个softmax层。通过这种方式，我们得到了最终输出。</p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230719145232272.png" alt="image-20230719145232272"></p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>为解决动作识别领域模型速度与精度之间的难题，提出一个多模功能融合网络结合骨架序列和RGB模式的框架。</p>
<p>​		早期融合阶段,我们引入一个骨架关注模块项目单上的骨架序列RGB框架帮助RGB框架关注肢体运动区域。后期融合阶段,我们提出一个cross-attention模块融合框架特性和RGB功能利用的关系。</p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230707203634081-1688733397791-1.png" alt="image-20230707203634081"></p>
<p>​		然而,尽管RGB视频还包含时间信息,但它并不直接或全面与骨骼形态相比,因为它是容易受背景,因此RGB视频通道的时间信息是表1中称为“弱</p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230707225904671-1688741946092-3.png" alt="image-20230707225904671"></p>
<p>​		在早期融合阶段,我们进行从骨架序列投影到RGB框架作为一个attention mask指导RGB网络关注最丰富的地区涉及肢体的运动。此外,self-attention也介绍了RGB框架网络抑制背景的信息。融合阶段后期,我们执行一个cross-attention融合模块一起把RGB的骨架特征的特征。</p>
<h1 id="贡献"><a href="#贡献" class="headerlink" title="贡献:"></a>贡献:</h1><p>​	1.(使用RGB帧)结合多模信息,我们融合一个RGB帧包含humanobject交互,而不是处理整个RGB视频与骨架序列,大大减少了计算消费以及维护性能。</p>
<p>​	2.（二阶段融合加上两个注意力模块）我们执行一个新的两阶段特征融合网络结合的知识RGB和骨架形式,介绍了这两个注意力机制组件来帮助网络集中在人造物交互区域和探索通信。</p>
<p>​	3.（先进有效）综合实验是进行南大RGB + D和于中山数据集。消融实验演示了该方法的有效性。结果表明,网络达到竞争性能与其他先进的方法相比,降低了网络的复杂性。</p>
<p>本文的其余部分组织如下,第二部分探讨了相关工作。第三节介绍了提出多模融合网络。第四节描述了我们的实现细节和实验结果。第五节提供了我们文章的结论。</p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>多模态：多模融合模块用于融合骨架特征和图像模态通过探索相关性，不同于以前使用future concatenation和pose-guided信息。</p>
<h1 id="技术方案"><a href="#技术方案" class="headerlink" title="技术方案"></a>技术方案</h1><h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><p>骨架流：</p>
<p>​		优点：时间信息好</p>
<p>​		缺点：与物体的交互信息差，空间信息不足</p>
<p>RGB流：</p>
<p>​		优点：有丰富的空间信息，有人与物体的交互信息</p>
<p>​		缺点：时间信息相比于骨架流不足</p>
<p>上述对比未证明</p>
<p>互补状态：</p>
<p>​		把这两个互补的模式识别结合。RGB视频中RGB帧的中间帧在神经网络中进行处理，然而,在第四节我们确认帧的中间部分视频也有效。</p>
<p>神经网络：</p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230708212822409-1688822905187-1.png" alt="image-20230708212822409"></p>
<p>骨架流由ST-GCN或三层Bi-LSTM网络进行处理（可换为其他骨架处理模型）</p>
<p>RGB流部分由Xception网络处理	(可改为其他RGB处理模型)</p>
<p>特征融合：</p>
<p>​		在RGB流中,骨架流将作为引导肢体运动位置的注意力模块以及早期特征融合阶段。self-attention模块被用于关注显著的区域。注意模块的输出都连接在一起,生成RGB特征映射。骨架特征映射和RGB特性映射通过后期融合融合模块获得最终结果。</p>
<p>​		提取RGB特性。具体来说,在RGB流,两个关注组件执行:self-attention模块是用来帮助网络关注人体肢体,而骨架注意力模块关注RGB帧上的骨架特征,作为早期功能融合阶段,帮助神经网络关注RGB帧里的人类肢体运动区域。在最后阶段,通过后期特征融合模块融合骨架流和RGB流。</p>
<h2 id="骨架流"><a href="#骨架流" class="headerlink" title="骨架流"></a>骨架流</h2><p>​		在骨架流中,我们使用两种不同的模型为骨干网络提取骨架序列特征。GCN-based的ST-GCN [56] 与而RNN-based的Bi-LSTM[36]。</p>
<p>st-gcn简略介绍：</p>
<p>​		ST-GCN,骨架序列构造空间时间作为一个无向图G &#x3D; (V, E)与N关节和T帧体内和frame连接。图中,节点包含所有骨骼关节和被表示为V &#x3D; {vti | t &#x3D; 1,2,……T i &#x3D; 1,2,……N}。的边缘设置由两个子集,哪一个是人体结构的连接在一个框架,另一个是每个关节的关节的连接在连续帧。ST-GCN采用类似图卷积操作[20]提取空间时间特点:</p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230708215505354-1688824506620-3.png" alt="image-20230708215505354"></p>
<p>LSTM的简略介绍：</p>
<p>​		LSTM[11]可以处理在各个步骤的序列数据。与RNN相比,LSTM可以学习远程依赖[47]和避免梯度的问题消失。典型的LSTM神经元包含一个输入门it,忘记门ft，单元状态ct,输出门ot, ht和输出响应。LSTM过渡方程可以表示为:</p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230708220052497-1688824854797-5-1688824856501-7.png" alt="image-20230708220052497"></p>
<h2 id="数据增强（Data-Enhancement）"><a href="#数据增强（Data-Enhancement）" class="headerlink" title="数据增强（Data Enhancement）"></a>数据增强（Data Enhancement）</h2><p>​		在处理骨架序列和单一RGB帧中,骨架序列只有一个视角。此外,人体在RGB视频中的RGB图像只占一小部分。这两种情况下会限制识别性能,因此我们引入措施,用数据扩充（data augmentation）加强骨骼数据和用投影作物（projection crop）加强RGB数据。</p>
<p><strong>数据扩充（Data Augmentation）</strong> 数据在我们的实现中结合数据增强基于旋转矩阵和scale-changing。如图3所示,南大RGB + D和于中山数据集是有限的视角。在南大RGB + D数据集,骨架3不同程度不同的视角:0度相机,+ 45度和- 45度相机。于中山数据集,只有存在数据为0度对相机。提高鲁棒性,我们提高了数据通过旋转矩阵。旋转矩阵公式如下,α,β和γ为顺时针方向旋转学位站在x, y, z轴。</p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230708221840674.png" alt="image-20230708221840674"></p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230708221926903-1688825968167-11.png" alt="image-20230708221926903"></p>
<p>三个旋转矩阵相乘后,我们得到:</p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230708222014124-1688826015686-15.png" alt="image-20230708222014124"></p>
<p>在我们的实现中,α和β的0度与30度之间的不同。此外,我们涉及尺寸变化的方法来进一步提高变化:</p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230708222241738-1688826163907-17.png" alt="image-20230708222241738">sx、sy、sz都是x、y、z轴scale-changing的因子，在我们的实现中,sx, sy 1和1.2之间变化,扩展数据集到五倍。通过旋转矩阵法和scale-changing方法,各种原始数据集得到提升。</p>
<p><strong>投影裁剪（Projection Crop）</strong>在RGB视频流中,人类在整个RGB图像中只占一个小区域。为了解决这个问题,我们提出一个投影裁剪图像的预处理方法。这个过程根据相应的骨骼数据帧人的边界框裁剪出人类对象以及附近的区域。假设w和h边界框的宽度和高度,我们使用边界框的四角作为起点和使用w + w′和h + h′裁剪原始图像, w′h′随机从100到300像素。因此图像增强4倍的数量。</p>
<p>​		通过投影裁剪方法,考虑到三维骨架坐标和摄像机参数,三维骨架序列被投影到RGB帧上。然后骨架序列和RGB框架之间的关系可以发现,2 d像素坐标可以通过投影方程计算。最后,人的区域可以确定。总体而言,该方法是一种常用的变体作物方法在图像或视频为基础的任务。该方法用人当作中心中心投影裁剪图像,而其他方法使用图像中心随机裁剪。提出该裁剪方法兼容骨架注意于两种模态相同的坐标,它可以被视为整个方法本身的一部分。该投影的优点是双重的:(1)人类的部分占用大部分的图片,这降低了背景的影响;(2)图像的中心在人类是固定的,这使得骨架的注意力容易被应用到图像。然而,随着其他fusion-based动作识别方法使用RGB视频提取的空间信息,这将是更复杂的执行投影作物每一帧,帧的坐标对齐可能影响视频的完整性表示。</p>
<h2 id="RGB流"><a href="#RGB流" class="headerlink" title="RGB流"></a>RGB流</h2><p>​		在我们的实现中,RGB流包括三个部分:基本卷积层,self-attention模块和框架关注模块。Xception[8]网络作为基本卷积层提取特征图。self-attention模块和框架关注模块用于生成关注权重。</p>
<p>​		<strong>Self-Attention模块</strong>。受到人员再识别中提取身体部位特征的方法的启发[64]，我们提出了一个自注意力模块来处理原始RGB帧的特征图。</p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230713175142266-1689241904702-1.png" alt="image-20230713175142266"></p>
<p>图4  Self-attention模块。这个模块需要将Xception从RGB帧提取的特征图作为输入。self-attention模块有两个重复的分支虚线框,用于生成注意模块的权重。在虚线框中,“1×1 Conv”表示1×1卷积层,⊗意味着矩阵乘法,“池”层代表全局平均池化层,和“线性”层代表班轮转换从而降低维数到256。每个self-attention箱的输出尺寸256,所以在“Concat”层,输出维度达到512。</p>
<p>​		self-attention模块旨在从RGB帧中提取动作的视觉显著性,即。强调人类的特性和抑制背景的特点。自注意力模块与在图像描述任务中使用的原始软注意力模块[55]有所不同，具体如下：<img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230713181503956-1689243306448-3.png" alt="image-20230713181503956"></p>
<p>​		fatt表示注意力模块,这是一个多层感知机。t代表了时间步骤当 i 代表了不同的图像的位置。ai表示图像特征和ht−1是隐藏的上下文状态的前一个时间步骤。ati是最后的注意力权重,这是一个概率与指数在0和1之间获得通过将softmax层。</p>
<p>​		在动作识别任务中，我们没有像图像标题分配那样的上下文信息。因此，隐藏状态 ht−1 和时间步长 t 不可用。如图所示。如图4所示，一个1×1卷积层的Conv被用来代替fatt注意力模块。对于输入特征图Fin ∈ RC×W ×H，其中W和H表示特征图的宽度和高度，1×1卷积用于生成注意力掩码Msel f ∈ R1×W ×H。sigmoid 函数σ用于替换 softmax 函数，以将注意力权重概率集中在 [0， 1] 范围内：</p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714020849245-1689271731042-3.png" alt="image-20230714020849245"></p>
<p>然后将输出特征 <img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714021010471-1689271812470-5.png" alt="image-20230714021010471">计算公式为：</p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714021103115-1689271864394-7.png" alt="image-20230714021103115"></p>
<p>其中⊙表示逐元素乘法。</p>
<ul>
<li><input disabled type="checkbox"> <strong>骨架注意力模块</strong></li>
</ul>
<p>总结:通过attention mask注意于人移动最大的关节的位置用来提取人与物的交互信息。</p>
<p>如图5所示 Skeleton Attention Weights但是如何生成未提及（热图是否可行）应该是不止是0或1（<img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714024758155-1689274082684-15.png" alt="image-20230714024758155">与<img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714155658072.png" alt="image-20230714155658072" style="zoom:67%;">的关系是什么？)</p>
<p>在该模块中，单个RGB帧和骨架序列组合为早期融合阶段。由于静态RGB帧不包含时间信息，因此使用骨架序列引导图像聚焦在移动的人-物体区域作为补充就足够了。</p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714022736278-1689272857775-9.png" alt="image-20230714022736278"></p>
<p>图5 骨架注意力权重的生成过程。第一个坐标系代表骨架序列，第二个坐标系代表最中间的 RGB 帧。骨架序列投影到最中间的 RGB 帧。注意力模块通过此过程推测最有趣的部分。例如，在上面显示的特定左第一图像中，通过在骨骼注意力模块中进行计算，人的左手具有最大的移动距离。他左手的黄色部分是我们添加的attention mask。然后，通过将注意力mask调整为裁剪后的图像来获得最终的骨架注意力权重。</p>
<p>使用骨架序列来引导图像聚焦在移动的人-物体区域作为补充就足够了</p>
<p>首先，具有最大移动距离dmax的骨架关节计算如下：</p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714023305963-1689273187402-11.png" alt="image-20230714023305963"></p>
<p>​		其中 J1 和 Jmiddle分别代表骨架和中间 RGB 帧的 3D 关节位置。通过计算移动距离，选出jmax变化最大的关节索引。其次，我们开始生成骨架注意力面具Mske ∈ R^(1×W ×H )。</p>
<p>​		生成骨架注意力mask时， 在以 jmax 为中心的正方形中注意力权重 <img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714024758155-1689274082684-15.png" alt="image-20230714024758155">&#x3D; 1，而在其他位置的 <img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714024740291-1689274062183-13.png" alt="image-20230714024740291"> &#x3D; 0，p 表示掩码的像素位置。然后，mask注意力与调整后的特征图大小相同，并生成骨架注意力权重。最后，通过骨架注意力权重和输入特征图Fin的逐元乘法得到人物相关特征Fske∈R^(C×W ×H)。</p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714135343094.png" alt="image-20230714135343094"></p>
<p>​		其中⊙表示逐元素乘法。产生骨架注意力的过程可以看作是在相对先验的阶段从骨架序列到单个RGB帧的投影。如图5所示人的左臂可以通过骨架注意力面罩来强调。通过这种方式，RGB流可以在骨架运动信息的指导下提取人与物体的交互特征。</p>
<h2 id="后期融合模块"><a href="#后期融合模块" class="headerlink" title="后期融合模块"></a>后期融合模块</h2><p>​		利用骨架信息引导RGB注意力后，将两个子网流（骨架流和RGB流）的特征组合起来进行动作分类。由于传统的决策融合对数据集的依赖性太大，并且受到多流融合方法[34] [65] [10]的启发，因此应用特征融合来帮助利用两种模态之间的互补信息。</p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714154013838.png" alt="image-20230714154013838"></p>
<p>图6 我们提出的多模态特征融合（MMFF）模块的架构。骨架特征和 RGB 特征按元素连接，然后发送到模块中。T 表示转置，Conv2D 表示 2D 卷积层，⊗ 代表矩乘法,此外 Softmax 是激活函数。融合模块的输出被发送到一个二维卷积层、全局平均池化层和两个 FC 层，以获得最终的特征嵌入。</p>
<p>​		由于ST-GCN和堆叠Bi-LSTM的特征图维度不同，分别表示为<img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714172431954-1689326674078-1.png" alt="image-20230714172431954">和<img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714172452395-1689326693535-3-1689326799199-5.png" alt="image-20230714172452395">，其中CS为通道维数，T为时间长度，V为骨架节点数，我们以不同的方式实现基于LSTM的融合和基于GCN的融合。</p>
<p><strong>基于 LSTM 的融合模块。</strong></p>
<p>​		由于骨架流的特征只用维CS压缩，采用后期融合策略来融合RGB特征FRGB。RGB 特征是通过结合自注意力特征 <img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230719131301188.png" alt="image-20230719131301188"> 和骨架注意力特征 生成的。两个特征首先通过MaxPooling层得到维数为C的特征，然后连接形成RGB流的特征<img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230719141703250.png" alt="image-20230719141703250"></p>
<p>​		为了实现骨架特征和 RGB 特征的最终特征融合，首先，我们将两个流的特征连接起来。其次，我们在级联特征中添加一个 L2 归一化层。然后，使用具有leaky ReLU的全连接层作为激活函数。该图层旨在查找时态骨架要素与空间 RGB 要素之间的非线性关系。最后两层是用于分类的完全连接的softmax层。</p>
<p><strong>基于GCN的融合模块。</strong></p>
<p>​		为了更好地探索两种模态之间的关系，并结合互补信息，我们设计了一个充分考虑特征特点的融合模块。对于骨架特征<img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714205404183-1689339246300-1.png" alt="image-20230714205404183">，它包含时空特征，而对于RGB特征<img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714205423532-1689339265282-3.png" alt="image-20230714205423532">，它仅包含空间特征。在融合模块中，由于骨架注意力已经帮助RGB流提取了关键信息，因此我们专注于利用跨模态的空间关系，如图6所示。</p>
<p>​		对于 RGB 特征，将两个注意力特征相加以平均相加以得到 RGB 特征 <img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714210843707-1689340125189-5.png" alt="image-20230714210843707"> ，其中 S 是 H ×W 的乘法。对于骨架特征，我们使用最大池化来变换仅包含空间特征 <img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714210931666-1689340172998-7.png" alt="image-20230714210931666"> 的特征。然后使用全局平均池 （GAP）将 RGB 特征和骨架特征转换为向量<img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714211432322-1689340473489-9.png" alt="image-20230714211432322"> 和<img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714211450412-1689340491938-11.png" alt="image-20230714211450412">。逐元素串联用于组合这两个功能。对于 RGB 特征，骨架矢量<img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714211945155-1689340786867-13.png" alt="image-20230714211945155"> 串联到 <img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714212016811-1689340818861-15.png" alt="image-20230714212016811">的每个通道特征，而对于骨架特征，RGB矢量<img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714212016811-1689340818861-15.png" alt="image-20230714212016811">串联到<img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714211945155-1689340786867-13.png" alt="image-20230714211945155"> 的每个通道特征。因此两个特征的通道维数相等，然后沿通道维连接得到组合特征<img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714212730282-1689341252054-17.png" alt="image-20230714212730282">。组合特征的形状为 （CS + CR） × （S + V)。</p>
<p>​		由于组合特征包含来自骨架流和 RGB 流的空间特征，我们使用两个 1 × 1 卷积层 Conv 来探索跨空间关系并生成关系掩码<img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714221102518-1689343864019-19.png" alt="image-20230714221102518">:</p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714221131812-1689343893228-21.png" alt="image-20230714221131812"></p>
<p>其中σ表示 softmax 函数。然后获得输出特征，如下所示：</p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230714222259503-1689344581346-23.png" alt="image-20230714222259503"></p>
<p>其中⊙表示逐元素乘法。然后，我们将关系特征图传递到一个GAP层和两个全连接层，然后是一个softmax层。通过这种方式，我们得到了网络的最终输出。</p>
<p><strong>训练步骤</strong></p>
<p>交叉熵损失用于优化网络，训练步骤如下：</p>
<p>1.对于图中的整体架构。去掉多模态融合模块，通过在顶部增加一个全连接（FC）层和一个softmax层，将两个流子网组成独立的网络。</p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230708212822409-1688822905187-1.png" alt="image-20230708212822409"></p>
<p>2.这两个子网是独立训练的，除了 FC 和 softmax 层之外，它们的权重被保存。</p>
<p>3.基于得到的权重，以这种方式固定两个流子网络的权重，并训练多模态融合模块。然后整个网络一起微调。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>我们对两个流行的视频动作识别数据集NTU RGB+D数据集和SYSU数据集进行了训练和测试程序，详细描述如下： 		**NTU RGB + D数据集[37]**：该数据集包含56880个视频样本中的60个不同的动作类，涵盖骨架，深度，红外和RGB视频模式。在我们的实现中，我们只挑选骨架时间模态和 RGB 视频空间模态数据源。此外，50节课由单个科目执行，其余10个类由两个科目执行的相互动作，其中单和双主体动作由25个关节组成。有两种标准评估协议，跨主体评估将 40 名受试者分为训练组和测试组，以及跨视图评估，利用相机 2 和 3 的样本进行训练，而相机 1 的样本进行测试。</p>
<p>​	**SYSU数据集[12]**：该数据集包含40名受试者和20个关节执行的12个动作。SYSU包含480个序列，其中所有序列都是关于人与物体交互动作的。有6个物体，即手机，椅子，背包，钱包，杯子，扫帚，拖把。还有两个标准评估协议设置：对于 Setting-1，一半的操作序列用于训练，其余用于测试。对于设置-2，一半的受试者用于训练，其余的用于测试。对于每种设置模式，将实现 30 倍交叉验证。</p>
<h2 id="实施详细信息"><a href="#实施详细信息" class="headerlink" title="实施详细信息"></a>实施详细信息</h2><p>​		该模型由 PyTorch 作为后端实现。它在四个Nvidia GTX 1080Ti GPU上训练。我们选择<strong>Adam</strong>作为优化器。学习率设置为 <img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230715140837277-1689401319061-1.png" alt="image-20230715140837277">，每 10 个 epoch 乘以 0.1。我们引入坐标变换来校准数据集。在我们的实现中，为了消除相机传感器位置和动作位置的影响，采用了**VA-pre [53]**的预处理方法。通过这种预处理方法，将坐标系的原点转换为第一帧的体中心。在下面的帧中，骨架的坐标根据其中心朝向原始点的相对位置进行稳定。</p>
<p>​		在 RGB 流中，输入图像的大小调整为 299 × 299。骨架序列被下采样到数据集的最小长度：从NTU RGB+D数据集中挑选出32帧，从SYSU数据集中挑选出58帧作为骨架序列。对于<img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230715141028039-1689401429526-3.png" alt="image-20230715141028039">主干，在NTU RGB+D数据集中骨架特征的维度设置为150，因此<img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230715141028039-1689401429526-3-1689401449776-5.png" alt="image-20230715141028039">的时间特征维度为600。对于 ST-GCN 主干，通过填充剪辑，两个数据集的时间维度都设置为 300。因此，NTU RGB+D 数据集的输出维度为 256 × 75 × 25，中山大学的输出维度为 256 × 75 × 20。使用 LSTM 方法的 RGB 特征的维度为 256 × 3 &#x3D; 768，因为有三个相同维度的部分：两个分支用于self-attention特征，一个分支用于骨架注意力特征。对于 RGB 帧，我们选择最中间的 RGB 帧作为 RGB 模态的表示形式。</p>
<h2 id="与最新方法的比较"><a href="#与最新方法的比较" class="headerlink" title="与最新方法的比较"></a>与最新方法的比较</h2><h3 id="精度比较。"><a href="#精度比较。" class="headerlink" title="精度比较。"></a>精度比较。</h3><p>我们的模型的精度如表2和表3所示，与大多数最新的最新方法相比。</p>
<p>​		<strong>NTU RGB+D的结果</strong>。我们将NTU RGB + D数据集上的跨主题和跨视图准确性与SOTA方法进行了比较，包括基于骨架的方法，如ST-LSTM + Trust Gate [37]，VA-LSTM [58]，D-Pose Traversal Conv [49]，ST-GCN [56]，DGNN [39]，MS-G3D Net [31]，基于RGB的方法，如Glimpse Clouds [1]，多模态方法，如2流RNN &#x2F; CNN [65]，深度双线性[14]， Posemap [30]，MFAS [33]，SGM-Net [23]，MMTM [17]，JOLO-GCN [4]。</p>
<p>​		根据实验结果，基于GCN的骨干自然比基于RNN的骨干更强大，因为它们在推理人类肢体运动之间的区域关系方面具有更高的能力。结果表明，MMFM在计算工作量增加最小的情况下明显优于单模态方法。例如，MMFF在跨主题评估中的成就比EleAtt-GRU [60]高4.7%，只有8.6G的FLOP。</p>
<p>​		公平地说，我们专注于将提出的MMFF与使用相同骨干的骨架流的方法进行比较。与以Bi-LSTM为骨干的2个流RNN&#x2F;CNN [65]相比，采用Bi-LSTM的MMFF取得了有竞争力的结果。至于以ST-GCN为骨干的SGM-Net [23]，采用ST-GCN的MMFF在两次评估中都具有出色的性能，参数和FLOP要少得多。这样的性能表明，尽管SGM-Net同时利用了骨架序列和整个RGB视频模式，但我们仅使用具有两阶段特征融合的单帧的MMFF模型在性能和计算消耗方面仍然超过它。此外，与最近的融合方法MMTM [17]和MFAS [33]相比，它们使用膨胀的ResNet50 [1]进行视频处理，HCN [22]进行骨架处理，MMFF仅以MMTM的11.4%的FLOP和MFAS的11.1%的FLOP实现了有竞争力的结果。至于Posemap [30]和JOLO-GCN [4]，其结果具有相同的主干ST-GCN，MMTM取得了竞争性的结果。对于Posemap，参数和FLOP未在论文中报告。由于 Posemap 使用 CNN 提取 RGB 视频的特征，因此计算工作量可能相对较大。JOLO-GCN提出构建围绕每个关节的光流运动以形成图形，并使用轻量级GCN提取特征。尽管JOLO-GCN的参数和FLOP小于所提出的MMFF，但并未考虑将图像处理为光流和联合引导光流图的消耗。</p>
<p>​		<strong>中山大学的结果</strong>。</p>
<p>​		至于SYSU数据集，我们将SYSU数据集上的Setting-1和Setting-2与基于骨骼的方法VA-LSTM [58]，DPRL+GCNN [43]，Local+LGN [19]，EleAtt-GRU [60]，LSGM + GTSC [15]，SGN [59]以及融合方法MTDA [63]，JOULE [13]，Deep-Bilinear [14]，PI3D [51]等方法进行了比较。可以看出，所提出的同时使用Bi-LSTM和ST-GCN的方法取得了有竞争力的性能。具体来说，使用ST-GCN的MMFF在SYSU的Setting-2中比多模态融合方法焦耳[13]高出2.2%，只有焦耳的12.0%。与使用I3D [5]进行视频处理的PI3D [51]相比，MMFF在设置2评估方面也取得了卓越的性能。虽然EleAttGRU [60]在Setting-1评估中实现了SOTA性能，但它是在NTU RGB + D上预先训练的。</p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230715142758295-1689402480627-7.png" alt="image-20230715142758295"></p>
<h3 id="复杂性比较"><a href="#复杂性比较" class="headerlink" title="复杂性比较"></a>复杂性比较</h3><p>​		当前的多模态方法要么应用基于LSTM的网络，要么基于GCN的网络作为骨干来处理骨架模态。通常，多个BiLSTM [36]（2.0M参数和6.3G FLOP）在基于LSTM的方法中实现最佳性能，而ST-GCN [56]（3.1M参数和15.6G FLOP）在基于GCN的网络中最受欢迎。我们的MMFF方法还利用上述两个骨架从骨架序列中提取时间信息。因此，骨架流的复杂性在当前方法中是相似的。对于RGB流，最先进的融合方法执行具有46.8M参数和168G FLOP的膨胀ResNet50[1]或具有12.1M参数和107.9G FLOP的C3D [45]以获得空间信息。然而，我们的MMFF通过Xception网络从单个RGB帧中提取空间表示，具有22.86M参数和8.42G FLOP，这大大节省了计算消耗。对于融合阶段，大多数融合方法只需要2-3 M参数，其差异可以忽略不计。总之，我们的方法在效率上超过了其他方法，这主要归功于单RGB帧的选择。</p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230715143105047-1689402666936-9.png" alt="image-20230715143105047"></p>
<p>​		例如，与 2 个流 RNN&#x2F;CNN [65] 相比，我们的方法表现要好得多，仅使用了 2 个流 RNN&#x2F;CNN 的 32.6% 和 63.5% 的参数和 FLOP。此外，我们的MMFF方法与MMTM [17]具有竞争性，MMTM使用ST-GCN和I-ResNet50作为骨干，具有49.4M参数和215.5G FLOP，而我们基于ST-GCN的模型只需要29.1M参数（MMTM的49.6%）和24.5G FLOP（MMTM的11.4%）。至于SOTA方法PI3D [51]，MMFF具有复杂度低的优点，因为PI3D使用I3D [5]进行RGB视频处理，其中I3D的参数和FLOP分别为12.2M和55.9G。因此，我们的方法与最先进的方法相比实现了有竞争力的性能，但显着节省了计算消耗。</p>
<h2 id="消融研究"><a href="#消融研究" class="headerlink" title="消融研究"></a>消融研究</h2><p>在本节中，我们将介绍消融研究，以测试NTU RGB + D和SYSU数据集上的方法有效性。消融研究包括RGB帧选择周期的选择、帧数的选择、数据增强的有效性、自我注意的有效性以及两阶段融合方法的有效性以及骨架在处理骨架序列中的骨干。</p>
<p>​		RGB 帧时间段的选择。在 RGB 流中，我们挑选单个 RGB 帧而不是整个视频剪辑以提高效率。我们进行了消融实验，以测试在NTU RGB + D和SYSU上使用LSTM和ST-GCN骨干网选择哪个时间段最合适。时间段的百分比范围为 10%、20%、30%,…到 90%。图列出了每个不同RGB帧的特征融合模块的精度结果。7. 结果表明，RGB视频中间部分的帧达到了相似的性能。由于实验中30%到70%的帧选择差异很小，因此单个RGB帧的选择非常灵活，这证明了我们的假设，即具有人机交互的单个RGB帧覆盖了足够的空间信息。为了统一标准以方便起见，我们提取了 50% 的中间 RGB 帧作为 RGB 特征表示。</p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230715144747670-1689403670159-11.png" alt="image-20230715144747670"></p>
<p>​		帧数的选择。我们在NTU RGB+D上进行实验，以找出帧数，性能和参数之间的关系。我们之所以选择位于视频 30%、40%、50%、60%、70% 的帧，是因为图 7 显示该模型在该区域中挑选的帧实现了最佳性能。结果表明，随着帧的添加，性能略有提高，而参数随着Xception [8]网络的多次使用而显著提高。因此，我们决定只使用单个帧来平衡效率。与 [35] 只从 RGB 模态学习时态信息不同，我们可以从骨架流中学习时态信息，因此考虑到计算消耗，我们最好选择不超过一帧。</p>
<p>4.4.3	数据增强的有效性。数据增强包括数据增强和投影裁剪，分别用于骨架和RGB。我们在NTU RGB + D和SYSU数据集上进行了以ST - GCN [ 56 ]为主干网络的实验，以测试我们提出的数据增强技术的有效性。对于无数据增强的Xception [ 8 ]、MFAS [ 33 ]和MMFF方法，采用相同的常用裁剪方法，将视频随机裁剪为大小为224 × 224 patch。 </p>
<p>​		如表5所示，数据增强合理地提高了网络的准确性。具体来说，对于骨架模态，数据增强有助于基线ST - GCN [ 56 ]在NTU RGB + D上的Cross - Subject提升0.8 %，表明提供不同的视点增强确实有助于模型获得更鲁棒和令人满意的结果。此外，在仅有RGB流的情况下，我们测试了经过Xception网络处理后的ProjCrop的有效性。可以看出，在SYSU的Setting - 1中，ProjCrop将RGB图像的性能提升了10.2 %，这进一步证明了我们的假设，过多的散乱背景可能会干扰RGB帧模型的性能，而帧上的作物在一定程度上可以帮助模型更专注于人体。 </p>
<p>表5 .在数据增强的实现上进行消融实验，其中在骨架序列模态上进行数据增强，在RGB图像模态上进行数据增强。Xception表示处理单个RGB帧的基线网络。在最后一行，将数据增强加入到整个网络中。我们使用MFAS的代码对Cross - View评价进行训练和测试。 </p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230719112351079-1689737034617-1.png" alt="image-20230719112351079"></p>
<p>当加入到整个网络MMFF中时，数据增强始终有助于性能的提升。 </p>
<p>​		为了进一步评估数据增强的效果，我们在骨架序列模态数据增强的MFAS [ 33 ]上进行了实验。实验结果表明，在NTU RGB + D数据集上，数据增强有助于提高原始MFAS的性能。通过在ST - GCN上的性能比较，证明了数据增强对于当前基于骨架的方法是有效的。此外，为了与MFAS进行公平的比较，我们测试了无数据增广的MMFF和无投影裁剪的MMFF。未进行数据增广的MMFF与MFAS仍具有竞争性能。也可以得出MMFF更多地受益于NTU RGB + D数据集上的投影作物，更多地受益于SYSU数据集上的数据增强。原因在于投影裁剪有助于减少NTU RGB + D数据集中图像的背景区域，数据增强可以针对相对较小的数据集增加训练数据以避免过拟合。 </p>
<p>4.4.4	自我注意机制的有效性。我们在NTU RGB + D和SYSU数据集上使用基于Bi - LSTM和STGCN的骨架进行实验，以测试自注意力机制的有效性。 </p>
<p>​		如表6所示，自注意力机制给骨干网带来了突出的提升。例如，在SYSU上的Cross - Subject中，自注意力模块将使用Bi - LSTM w &#x2F; o自注意力的MMFF的性能提升了1.8 %，证明了我们提出的自注意力模块帮助RGB流更专注于人-物交互区域，以提取更具代表性的信息。此外，在NTU RGB + D上的Cross - Subject实验中，骨架注意力将ST - GCN w &#x2F; o骨架注意力的MMFF提升了3.0 %，表明骨架注意力可以有效地帮助更好地融合两种模态，使模型更专注于从背景中区分前景。 </p>
<p>4.4.5	早期融合模块的有效性。我们使用基于Bi - LSTM和STGCN的网络进行实验，以证明我们提出的早期融合方法的性能。使用骨架注意力机制作为早期融合阶段，将时序知识迁移到RGB流中。如表6所示，在SYSU上，骨架注意力机制使Bi - LSTM提升了13.5 %；在NTU RGB + D上，骨架注意力机制使Cross - View提升了8.0 %。数据表明，骨架序列的投影注意力机制不仅引导RGB框架聚焦于人-物交互区域，而且作为融合的早期阶段，有助于进一步提高特征融合效果。不仅如此，我们还可以观察到骨架注意比自我注意发挥着更重要的作用。 </p>
<p>表6 .基于Bi - LSTM和ST - GCN骨干网的模块组合性能。SelfAttention表示自注意力模块，Skeleton - Attention表示骨架注意力模块。MMFF意味着对自我和骨架注意模块的利用。决策融合是朴素的加权和决策融合方法，和融合是将基于Bi - LSTM的方法的输出按照全连接层和softmax进行分类。 </p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230719114444803-1689738287076-5.png" alt="image-20230719114444803"></p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230719114521391.png" alt="image-20230719114521391"></p>
<p>图8 . SYSU数据集的混淆矩阵：( a ) Bi - LSTM模型的结果；( b ) MMFF模型的结果。结果是所有30倍交叉验证的设置- 1和设置- 2之和。 </p>
<p>4.4.6	后期融合模块的有效性。在我们的实现中，我们在NTU RGB + D和SYSU数据集上测试了基于LSTM和ST - GCN骨架的后期融合方法。我们将所提出的方法与决策融合和和融合方法的性能进行了比较。结果报告于表6。 </p>
<p>​		实验结果表明，在NTU RGB + D和SYSU数据集上，MMFF的精度均优于其他方法。对于基于Bi - LSTM和ST - GCN的方法，所提出的后期融合模块比决策融合方法的大部分结果都取得了一致的改进。例如，我们提出的Bi - LSTM上的后期融合模块在NTU RGB + D上的Cross - View和SYSU上的Setting - 1上分别比决策融合方法取得了5.0 %和3.8 %的性能提升。然而，SYSU Setting - 1评估的最佳性能是通过决策融合实现的。考虑到决策融合在更大的NTU RGB + D数据集上泛化效果不佳，可将结果视为异常。此外，在NTU RGB + D的Cross - Subject和SYSU的Setting - 1中，本文提出的后期融合模块的MMFF也分别超过了总和融合方法的2.4 %和2.0 %。 </p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230719120601774.png" alt="image-20230719120601774"></p>
<p>图9	典型动作在NTU RGB + D交叉学科上的表现。横轴上的数字表示动作标签。动作分别为”穿夹克” ( 14 )、”脱夹克” ( 15 )、”穿鞋” ( 16 )、”脱鞋” ( 17 )、”戴眼镜” ( 18 )、”脱眼镜” ( 19 )、”吃饭&#x2F;吃零食” ( 2 )、”读书” ( 11 )、”写字” ( 12 )、”玩手机&#x2F;平板电脑” ( 29 )、”键盘打字” ( 30 )、”打喷嚏&#x2F;咳嗽” ( 41 )。 </p>
<p>4.4.7	类别正确率分析。图8描述了基于混淆矩阵的Bi - LSTM MMFF在SYSU数据集上的有效性。矩阵中的元素表示将一种动作分类到其他动作的概率，对角线上的元素表示正确分类的概率。根据融合矩阵，我们观察到MMFF的对角线元素比骨干Bi - LSTM一致地提高，这表明MMFF有助于提高不同类的性能。由于SYSU中的动作主要包含人-物交互，本文提出的方法有利于融合空间信息进行动作识别。 </p>
<p>​		对于NTU RGB + D数据集，我们重点分析了性能下降和准确率较低的类别，而MMFF在其他动作上取得了一致的提升。如图9 ( a )所示，我们观察到6个动作的性能略有下降，因为Xception对具有相同对象和移动区域(例如,穿外套和脱外套)的动作分类有困难。对于ST - GCN的挑战性动作，MMFF可以在RGB图像模态的帮助下提高性能。 </p>
<p>4.4.8	骨架序列处理中骨架的选择。我们在NTU RGB + D和SYSU数据集上进行实验，以确定适合基于LSTM的网络的主干。如表7所示，在基于LSTM的方法中，三重Bi - LSTM取得了最高的性能。然而，由于基于GCN的方法取得了比基于LSTM的方法更好的性能，公平地说，与最先进的方法相比，基于LSTM和基于GCN的方法都被使用。 </p>
<h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><p>自注意力模块图和骨架注意力模块图分别如图10和图11所示。所有的图片都按照实现细节中的描述进行裁剪。可以看出，自注意力使模型更专注于人-物交互区域，而骨架注意力使模型更专注于人体肢体的运动。 </p>
<p>表7 .在NTU RGB + D和SYSU数据集上基于LSTM的方法的性能。 </p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230719124236576.png" alt="image-20230719124236576"></p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230719124258622.png" alt="image-20230719124258622"></p>
<p>图10 .自我注意力的热力图。第一列倾向于关注身体部分，而第二列则关注背景。 </p>
<p><img src="/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/image-20230719124349774.png" alt="image-20230719124349774"></p>
<p>图11 .骨架注意力机制的说明。表明实现的骨架注意力机制有助于网络更多地关注身体运动部分。 </p>
<h1 id="论述"><a href="#论述" class="headerlink" title="论述"></a>论述</h1><h2 id="适应不同的数据集"><a href="#适应不同的数据集" class="headerlink" title="适应不同的数据集"></a>适应不同的数据集</h2><p>​		提出的MMFF在SYSU和NTU RGB + D数据集上取得了一致的性能，且复杂度较低，但该方法表现出不同的数据集特性。由于SYSU数据集相对较小，网络从数据增强中获益更多。对于动作更具挑战性的NTU RGB + D数据集，引入带有MMFF的RGB帧模态有助于提升性能。虽然一些单模态方法取得了有竞争力的表现，但在其他评价指标(例如, SGN )下表现不佳，这说明了互补多模态尤其是对复杂动作识别的重要性。 </p>
<h2 id="MMFF的便利性和局限性"><a href="#MMFF的便利性和局限性" class="headerlink" title="MMFF的便利性和局限性"></a>MMFF的便利性和局限性</h2><p>​		MMFF的主要优点是训练和推理的便利性。与SOTA多模态方法相比，MMFF以更低的复杂度获得了更好或相当的性能。值得注意的是，虽然JOLO - GCN的复杂度较低，但对每个视频进行光流预处理需要1.5 s [ 4 ]，而MMFF进行预处理的时间可以忽略不计。主要的局限性在于，MMFF与基于SOTA骨架的利用运动和时间相关性开发轻量级GCN的方法相比，<strong>并没有表现出明显的优越性。</strong>然而，为了简单起见，我们提出了一种将RGB视频替换为RGB帧的融合方法，该方法易于与其他基于骨架的方法结合。基准GCN模型用于与多模态方法进行公正的比较，更有效的GCN模型可以与所提出的方法结合以提高性能。 </p>
<h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>本文提出了一种用于动作识别的两阶段多模态特征融合模型。我们的主要贡献是提取整个视频中间部分的一个RGB帧，以保持RGB流中的大部分关键信息，并提高效率。为了更好地学习两种模态的对应关系，我们将骨架注意力模块作为早期融合阶段，将时间知识从骨架模态转移到RGB模态，帮助模型在RGB框架上更多地关注人体肢体的运动区域。对于后期融合阶段，我们引入融合网络来更好地融合两种模态。在LSTM和ST - GCN主干网络上的实验结果表明，所提方法获得了比现有方法更好或更有竞争力的结果，并降低了复杂度。然而，在一些特定的应用中，如实时视频监控，这项工作可能会遇到困难，因此我们可以跟随这项工作，尝试新的方法将MMFF应用到这些应用中。 </p>

            </div>

            <!-- Post Comments -->
            
    <!-- 使用 DISQUS_CLICK -->
<div id="disqus-comment">
    <div id="disqus_thread"></div>

<!-- add animation -->
<style>
	.disqus_click_btn {
            line-height: 30px;
            margin: 0;
            min-width: 50px;
            padding: 0 14px;
            display: inline-block;
            font-family: "Roboto", "Helvetica", "Arial", sans-serif;
            font-size: 14px;
            font-weight: 400;
            text-transform: uppercase;
            letter-spacing: 0;
            overflow: hidden;
            will-change: box-shadow;
            transition: box-shadow .2s cubic-bezier(.4, 0, 1, 1), background-color .2s cubic-bezier(.4, 0, .2, 1), color .2s cubic-bezier(.4, 0, .2, 1);
            outline: 0;
            cursor: pointer;
            text-decoration: none;
            text-align: center;
            vertical-align: middle;
            border: 0;
            background: rgba(158, 158, 158, .2);
            box-shadow: 0 2px 2px 0 rgba(0, 0, 0, .14), 0 3px 1px -2px rgba(0, 0, 0, .2), 0 1px 5px 0 rgba(0, 0, 0, .12);
            color: #fff;
            background-color: #7EC0EE;
            text-shadow: 0
        }
</style>
	
<div class="btn_click_load" id="disqus_bt"> 
    <button class="disqus_click_btn">点击查看评论</button>
</div>

<!--
<script type="text/javascript">
$('.btn_click_load').click(function() {
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'http-miccall-tech'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    document.getElementById('disqus_bt').style.display = "none";
});
</script>
-->
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'https://blog.ldf.icu/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'https://blog.ldf.icu/2023/07/07/%E8%AE%BA%E6%96%87%E7%BB%86%E8%AF%BB_%E9%AA%A8%E6%9E%B6%E5%BA%8F%E5%88%97%E5%92%8C%E5%9F%BA%E4%BA%8ERGB%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%8A%9F%E8%83%BD%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>

<script type="text/javascript">
    $('.btn_click_load').click(function() {  //click to load comments
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document;
            var s = d.createElement('script');
            s.src = '//http-miccall-tech.disqus.com/embed.js';
            s.setAttribute('data-timestamp', + new Date());
            (d.head || d.body).appendChild(s);
        })();
        $('.btn_click_load').css('display','none');
    });
</script>
</div>
<style>
    #disqus-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Design: <a target="_blank" rel="noopener" href="http://miccall.tech " style="border-bottom: none;">miccall</a></li>
            </ul>
            
                <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span>
			
        </div>
    </div>
</body>



 	
</html>
